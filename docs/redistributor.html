<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>redistributor API documentation</title>
<meta name="description" content=":warning: | Still under development
:---: | :--- â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>redistributor</code></h1>
</header>
<section id="section-intro">
<table>
<thead>
<tr>
<th align="center">:warning:</th>
<th align="left">Still under development</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>This repository introduces two main classes, namely <strong>Redistributor</strong> and <strong>LearnedDistribution</strong>.</p>
<p><strong>Redistributor</strong> is a tool for automatic transformation of empirical data distributions. It is implemented as a <strong>Scikit-learn transformer</strong>. It allows users to transform their data from arbitrary distribution into another arbitrary distribution. The source and target distributions, if known beforehand, can be specified exactly (e.g. as a <a href="https://docs.scipy.org/doc/scipy/reference/tutorial/stats/continuous.html#continuous-distributions-in-scipy-stats">Continuous Scipy distribution</a> or any other class which has <code>cdf</code> and <code>pdf</code> methods implemented), or can be inferred from the data using <code><a title="redistributor.LearnedDistribution" href="#redistributor.LearnedDistribution">LearnedDistribution</a></code> class. Transformation is <strong>piece-wise-linear, monotonic, invertible</strong>, and can be <strong>saved for later use</strong> on different data assuming the same source distribution.</p>
<p><strong>LearnedDistribution</strong> is a subclass of <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.html#scipy-stats-rv-continuous">Scipy.stats.rv_continous</a> class. It is a continuous random variable obtained by estimating the empirical distribution of a user provided array of numeric data <code>x</code>. It can be used to sample new random points from the learned distribution.</p>
<!-- The empirical distribution can be inferred from a 1D array of data. To redistribute multiple slices of your data use <code>Redistributor\_multi</code> class which has a **low memory footprint** and utilizes **parallel computing** to apply multiple <code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code> objects. -->
<h2 id="installation">Installation</h2>
<table>
<thead>
<tr>
<th align="center">:warning:</th>
<th align="left">Not yet published on PyPi. Coming soon.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The code is hosted in this <a href="https://gitlab.com/paloha/redistributor">GitLab repository</a>.
To install the released version from Pypi use:</p>
<pre><code class="language-bash">pip install redistributor
</code></pre>
<p>Or install the bleeding edge directly from git:</p>
<pre><code class="language-bash">pip install git+https://gitlab.com/paloha/redistributor
</code></pre>
<p>For development, install the package in editable mode with extra dependencies for documentation and testing:</p>
<pre><code class="language-bash"># Clone the repository
git clone git@gitlab.com:paloha/redistributor.git
cd redistributor

 # Use virtual environment [optional]
python3 -m virtualenv .venv
source .venv/bin/activate

# Install with pip in editable mode
pip install -e .[dev]
</code></pre>
<h2 id="compatibility">Compatibility</h2>
<p>&hellip;</p>
<h2 id="dependencies">Dependencies</h2>
<p>Required packages for <code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code> are specified in the <code>install_requires</code> list in the <code>setup.py</code> file.</p>
<p>Extra dependencies for running the tests, compiling the documentation, or running the examples are specified in the <code>extras_require</code> dictionary in the same file.</p>
<p>The full version-locked list of dependencies and subdependencies is frozen in <code>requirements.txt</code>. Installing with <code>pip install -r requirements.txt</code> in a virtual environment should always lead to a fully functional project.</p>
<h2 id="mathematical-description">Mathematical description</h2>
<p>Assume we are given data <span><span class="MathJax_Preview">x\sim S</span><script type="math/tex">x\sim S</script></span> distributed according to some source distribution <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> on <span><span class="MathJax_Preview">\mathbb{R}</span><script type="math/tex">\mathbb{R}</script></span> and our goal is to find a transformation <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> such that <span><span class="MathJax_Preview">R(x)\sim T</span><script type="math/tex">R(x)\sim T</script></span> for some target distribution <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> on <span><span class="MathJax_Preview">\mathbb{R}</span><script type="math/tex">\mathbb{R}</script></span>.</p>
<p>One can mathematically show that a suitable <span><span class="MathJax_Preview">R\colon \mathbb{R} \to \mathbb{R}</span><script type="math/tex">R\colon \mathbb{R} \to \mathbb{R}</script></span> is given by
<span><span class="MathJax_Preview">
R := F_{T}^{-1} \circ F_{S},
</span><script type="math/tex; mode=display">
R := F_{T}^{-1} \circ F_{S},
</script></span>
where <span><span class="MathJax_Preview">F_S</span><script type="math/tex">F_S</script></span> and <span><span class="MathJax_Preview">F_T</span><script type="math/tex">F_T</script></span> are the cumulative distribution functions of <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> and <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>, respectively.</p>
<p>If <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> and <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is unknown, one can use approximations <span><span class="MathJax_Preview">\tilde{F}_S</span><script type="math/tex">\tilde{F}_S</script></span> and <span><span class="MathJax_Preview">\tilde{F}_T</span><script type="math/tex">\tilde{F}_T</script></span> of the corresponding cumulative distribution functions given by interpolating (partially) sorted data
<span><span class="MathJax_Preview">
(x_i)_{i=1}^N \ \text{with} \ x_i \sim S
</span><script type="math/tex; mode=display">
(x_i)_{i=1}^N \ \text{with} \ x_i \sim S
</script></span>
<span><span class="MathJax_Preview">
(y_i)_{i=1}^M \ \text{with} \ y_i \sim T.
</span><script type="math/tex; mode=display">
(y_i)_{i=1}^M \ \text{with} \ y_i \sim T.
</script></span>
Defining
<span><span class="MathJax_Preview">
\tilde{R} := \tilde{F}_{T}^{-1} \circ \tilde{F}_S,
</span><script type="math/tex; mode=display">
\tilde{R} := \tilde{F}_{T}^{-1} \circ \tilde{F}_S,
</script></span>
one can, under suitable conditions, show that
<span><span class="MathJax_Preview">
\tilde{R} \xrightarrow[N,M\to \infty]{} R.
</span><script type="math/tex; mode=display">
\tilde{R} \xrightarrow[N,M\to \infty]{} R.
</script></span></p>
<h2 id="how-to-cite">How to cite</h2>
<p>&hellip;</p>
<h2 id="license">License</h2>
<p>This project is licensed under the terms of the MIT license.
See <code>license.txt</code> for details.</p>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>This work was supported by the <em>International Mobility of Researchers</em> (program call no.: <a href="https://opvvv.msmt.cz/vyzva/vyzva-c-02-16-027-mezinarodni-mobilita-vyzkumnych-pracovniku.htm">CZ.02.2.69/0.0/0.0/16027/0008371</a>).
<img alt="opvvv" src="https://gitlab.com/paloha/redistributor/uploads/19903a1b9e00015faa2b61234a99b911/opvvv.jpg"></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
.. include:: readme.md
&#34;&#34;&#34;

from __future__ import division

import warnings
import numpy as np
from scipy.stats import rv_continuous
from scipy.interpolate import interp1d
from sklearn.base import TransformerMixin


class Redistributor(TransformerMixin):
    &#34;&#34;&#34;
    An algorithm for automatic transformation of data from arbitrary
    distribution into arbitrary distribution. Source and target distributions
    can be known beforehandand or learned from the data using
    LearnedDistribution class. Transformation is piecewise linear, monotonic
    and invertible.

    Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
    and saved to be used later for transforming other data assuming the same
    source distribution.

    Uses source&#39;s and target&#39;s `cdf()` and `ppf()` to infer the
    transform and inverse transform functions.

    `transform_function = target_ppf(source_cdf(x))`
    `inverse_transform = source_ppf(target_cdf(x))`
    &#34;&#34;&#34;

    def __init__(self, source, target):
        self.source = source
        self.target = target

    def fit(x=None, y=None):
        &#34;&#34;&#34;
        Redistributor does not need to be fitted.
        &#34;&#34;&#34;
        pass

    def transform(self, x):
        &#34;&#34;&#34;
        Transform the data from source to target distribution.
        &#34;&#34;&#34;
        return self.target.ppf(self.source.cdf(x))

    def inverse_transform(self, x):
        &#34;&#34;&#34;
        Inverse transform the data from target to source distribution.
        &#34;&#34;&#34;
        return self.source.ppf(self.target.cdf(x))

    def kstest(self, n=20):
        &#34;&#34;&#34;
        Performs the (one-sample or two-sample) Kolmogorov-Smirnov test.
        &#34;&#34;&#34;
        from scipy.stats import kstest
        return kstest(self.source.rvs, self.target.cdf, N=n,
                      alternative=&#39;two-sided&#39;, mode=&#39;auto&#39;)

    def plot_transform_function(self, bins=1000, newfig=True, figsize=(16, 5)):
        &#34;&#34;&#34;
        Plotting the learned transformation from source to target.
        &#34;&#34;&#34;
        import matplotlib.pyplot as plt
        x = np.linspace(*self.source._get_support(), bins)
        t = self.transform(x)
        if newfig:
            plt.figure(figsize=figsize)
        plt.title(&#39;Transform function&#39;)
        plt.plot(x, t)
        if newfig:
            plt.show()
            plt.close()
        return


class LearnedDistribution(rv_continuous):
    &#34;&#34;&#34;
        A continuous random variable obtained by estimating the empirical
        distribution of a user provided 1D array of numeric data `x`. It
        can be used to sample new random points from the learned distribution.

        It approximates the Cumulative Distribution Function (`cdf`) and
        Percent Point Function (`ppf`) of the underlying distribution of `x`
        using linear interpolation on a lattice.

        An approximation of the Probability Density Function (`pdf`) is
        computed as an interpolation of the numerical derivative of the `cdf`
        function. Please note it can oscilate a lot if `bins` is high.

        The distribution is defined on a closed finite interval `[a, b]` or
        `[xmin, xmax]` or combination thereof, depending on which bound/s
        were specified by the user.

        WARNING: It can not be used to learn discrete distributions.


        Parameters
        ----------

        x : 1D numpy array
            1D vector of which the distribution will be estimated.

        a : numeric or None
            Left boundary of the distribution support if known.
            If specified, must be smaller than x.min().

        b : numeric or None
            Right boundary of the distribution support if known.
            If specified, must be bigger than x.max().

        bins : int or None
            User specified value of bins. Min is 3, max is `x.size`.
            If None or 0, bins are set automatically. Upper bound
            is set to 1000 to prevent unnecessary computation.
            Used to specify the density of the lattice. More bins
            means higher precision but also more computation.

        keep_x_unchanged : bool, default True
            If True, the `x` array will be copied before partial sorting.
            This will result in increased memory usage. But it will
            not reorder the user provided array.

            If False, there will not be any additional memory consumption.
            But the user provided array `x` might change its order.
            This might be very useful if `x` is a large array and there is
            not enough available memory.

        subsample_x : int, default None
            Sacrifice precision for speed by first subsampling array `x`
            with a defined integer step. Not doing `random.choice()` but rather
            simple `slice(None, None, subsample_x)` because it is faster and
            we assume the array is randomly ordered. Can lead to significant
            speedups.

        ravel_x : bool, default True
            LearnedDistribution requires 1D arrays. So the `x` is by default
            flattened to 1D using `np.ravel()`.

        assume_sorted : bool, default False
            If the user knows that `x` is sorted, setting this to True will
            save a most of time by ommiting partial sorting the array.
            Especially useful if the array `x` is big. E.g. 1GB of data
            takes approx. 10s to partial sort on 5000 positions.
            If `False` and `x` is almost sorted, it will still be faster than
            if `x` is randomly ordered.

        fill_value : None, array-like, float, 2-tuple or &#39;auto&#39;, default=&#39;auto&#39;
            Specifies where to map the values out of the `cdf` support. See the
            docstring of scipy.interpolate.interp1d to learn more about the
            valid options. Additionally, this class enables the user to use
            the default `auto` option, which sets reasonable `fill_value`
            automatically.

        bounds_error : bool or &#39;warn&#39;, default &#39;warn&#39;
            See the docstring of class interp1d_with_warning.

        dupl_method : str, one of {&#39;keep&#39;, &#39;spread&#39;, &#39;cluster&#39;, &#39;noise&#39;}
                      default &#39;spread&#39;
            Method of solving duplicate lattice values. Read more in
            docstring of `make_unique()`.

        name : str, default &#39;LearnedDistribution&#39;
            The name of the instance.

        seed : {None, int, `numpy.random.Generator`,
            `numpy.random.RandomState`}, default None
            See the docstring of scipy.stats.rv_continuous.
            Used in `_prevent_same()` and `rvs()`.

        kwargs : all other keyword arguments accepted by rv_continous.


        Methods - TODO finish this documentation
        -------

        cdf
        ppf
        pdf
        rvs
        entropy
        ... fill in the rest which is implemented
        ... handle the rest which does not make sense
        &#34;&#34;&#34;

    def __init__(self, x, a=None, b=None, bins=None, keep_x_unchanged=True,
                 subsample_x=None, ravel_x=True, assume_sorted=False,
                 fill_value=&#39;auto&#39;, bounds_error=&#39;warn&#39;, dupl_method=&#39;spread&#39;,
                 seed=None, name=&#39;LearnedDistribution&#39;, **kwargs):

        super().__init__(name=name, seed=seed, **kwargs)

        if ravel_x:
            x = x.ravel()

        # Sacrifice precision for speed
        if isinstance(subsample_x, int):
            if 2 &lt;= subsample_x &lt;= x.size:
                x = x[::subsample_x]
            else:
                raise ValueError(&#39;Not 2 &lt;= subsample_x &lt;= x.size.&#39;)

        # Handling input data and interval
        self._validate_x(x)
        self.xmin = x.min()
        self.xmax = x.max()
        self._validate_a_b(a, b)
        self.a = a
        self.b = b

        # Arguments for interpolation
        self.bounds_error = bounds_error
        self.fill_value = fill_value

        # Setting lattice density
        self.bins = self._infer_bins(x.size, bins)

        # Interpolating to get the empirical distribution
        self.assume_sorted = assume_sorted
        self.dupl_method = dupl_method
        lattice, vals = self._get_lattice_and_vals(x, keep_x_unchanged)
        self._cdf = self._get_cdf(lattice, vals)
        self._ppf = self._get_ppf(lattice, vals)
        self._pdf = self._get_pdf()

    def _get_support(self, *args):
        &#34;&#34;&#34;
        Support of LearnedDistribution does not depend on any scipy arguments,
        we keep args only to keep the signature unchanged.

        The support depends only on whether `a` and/or `b` were specified
        explicitely or as Nones.

        `self.a` and/or `self.b` are kept stored as Nones to keep the
        information about the object config for future reference of the user.

        Returns
        -------
        a, b : numeric (float, or int)
            end-points of the distribution&#39;s support.
        &#34;&#34;&#34;
        return self._cdf.x[0], self._cdf.x[-1]

    def _get_support_ppf(self, *args):
        &#34;&#34;&#34;
        The support of `ppf` in scipy is always `[0,1]` so this method does not
        exist in `rv_continuous`. In our case, the support might be shrinked if
        any of the a, b is set to None.
        &#34;&#34;&#34;
        return self._ppf.x[0], self._ppf.x[-1]

    def _validate_x(self, data):
        &#34;&#34;&#34;
        Validation of the input data.

        Parameters
        --------
        data: numpy array of data to be validated
        &#34;&#34;&#34;
        if not np.issubdtype(data.dtype, np.floating):
            raise TypeError(&#39;Input array dtype must be floating point.&#39;)
        if np.issubdtype(data.dtype, np.float16):
            warnings.warn((
                &#39;Using float16 data can lead to large errors. &#39;
                &#39;Rather use f32 or f64.&#39;))
        if not data.ndim == 1:
            raise ValueError(&#39;Input array must be 1D. You can use x.ravel().&#39;)

    def _validate_a_b(self, a, b):
        &#34;&#34;&#34;
        Validation of the boundaries.

        Parameters
        --------
        a: numeric or None
            See the docstring of __init__().
        b: numerc or None
            See the docstring of __init__().
        &#34;&#34;&#34;
        if a is not None:
            if not np.isfinite(a):
                raise ValueError(f&#39;a {a} must be finite.&#39;)
            if a &gt;= self.xmin:
                raise ValueError(f&#39;a {a} must be &lt; than xmin {self.xmin}.&#39;)
        if b is not None:
            if not np.isfinite(b):
                raise ValueError(f&#39;b {b} must be finite.&#39;)
            if b &lt;= self.xmax:
                raise ValueError(f&#39;b {b} must be &gt; than xmax {self.xmax}.&#39;)

    def _infer_bins(self, n, bins):
        &#34;&#34;&#34;
        Infers and validates the number of bins.

        Parameters
        --------
        n: int
            Size of the data.

        bins: int or None
            See the docstring of __init__().
        &#34;&#34;&#34;
        if bins is None or bins == 0:
            bins = min(n, int(5e3))
        if bins &gt; n or bins &lt; 3:
            raise ValueError(f&#39;Bins ({bins}) must be 2 &lt; bins &lt;= x.size&#39;)

        return bins
    

    def _get_lattice_and_vals(self, x, keep_x_unchanged):
        &#34;&#34;&#34;
        Creating the `lattice` based on the number of `bins` and assembling the
        corresp. `lattice_vals` from provided array `x` using partial sort.

        Parameters
        --------
        x: 1D numpy array
            See the docstring of __init__().

        keep_x_unchanged: bool
            See the docstring of __init__().

        Returns
        -------

        lattice, 1D array of equidistant values
            Support of ppf or range of cdf. The first value of the array will
            be either 0 or epsilon and the last value will be either 1 or
            1 - epsilon depending if a or b are specified or None. Size or the
            array will range from bins to bins + 2.

        lattice_vals, 1D array
            Range of ppf or support of cdf. The first value of the array will
            be either xmin or a and the last value will be either xmax or b
            depending if a or b are specified or None. Size of the array will
            be the same as of `lattice`.
        &#34;&#34;&#34;

        # Do we need expansion by a or b from Left or Right side or both?
        L, R = self.a is not None, self.b is not None

        # Indices at which we need x to be sorted considering L and R
        indices = np.linspace(
            0, x.size + L + R - 1, self.bins).round().astype(int)
        indices = indices[L:-1 if R else None] - L

        if not self.assume_sorted:
            # Reorder values of x on indices as if the array was sorted
            if keep_x_unchanged:
                # Does not change x but uses twice the memory
                x = np.partition(x, indices)
            else:
                # Does not use additional memory but alters the order of data
                x.partition(indices)

        # Get the _ppf.y (or _cdf.x) values
        eps = 1 / (x.size + 1)  # Shrink the lattice with eps to exclude 0 or 1
        lattice_vals = np.hstack([[self.a] * L, x[indices], [self.b] * R])

        # Get the _ppf.x (or _cdf.y) values
        lattice = np.linspace(
            eps * (not L), 1 - eps * (not R), lattice_vals.size)

        if not all(np.isfinite(lattice_vals)):
            raise ValueError(&#39;Values of x on the lattice must be finite.&#39;)

        # If necessary, make duplicate values unique and warn the user.
        lattice_vals = make_unique(
            lattice_vals, self.random_state, mode=self.dupl_method)
        return lattice, lattice_vals

    def _get_cdf(self, lattice, lattice_vals):
        &#34;&#34;&#34;
        Interpolates the lattice on lattice_vals to get the `cdf`.

        Table of `fill_value` if `self.fill_value == &#39;auto&#39;` (n = x.size):
        ------------------------------------------------------------------
        a     | b     | cdf support | truncated to | fill_value
        ------------------------------------------------------------------
        None  | None  | xmin, xmax  | xmin, xmax   | 1/(n-1), (n-2)/(n-1)
        None  | b     | xmin, b     | xmin, None   | 1/(n-1), None
        a     | None  | a,    xmax  | None, xmax   | None, (n-2)/(n-1)
        a     | b     | a,    b     | None, None   | None, None
        &#34;&#34;&#34;

        if self.fill_value == &#39;auto&#39;:
            fill_value = (lattice[0] if self.a is None else None,
                          lattice[-1] if self.b is None else None)
        else:
            # User defined fill_value
            fill_value = self.fill_value

        return interp1d_with_warning(
            lattice_vals, lattice, kind=&#39;linear&#39;, assume_sorted=True,
            bounds_error=self.bounds_error, fill_value=fill_value)

    def _get_ppf(self, lattice, lattice_vals):
        &#34;&#34;&#34;
        Interpolates the lattice_vals on a lattice to get the `ppf`.
        `fill_values` is set to the `xmin` and `xmax` to avoid problems
        when generating random sample with `rvs()`. `bounds_error` is
        set to `False` because user does not need a warning about this
        behaviour and values `q &lt; 0` or `q &gt; 1` should not ever occur at all.
        &#34;&#34;&#34;
        fill_value = (lattice_vals[0], lattice_vals[-1])
        return interp1d_with_warning(
            lattice, lattice_vals, kind=&#39;linear&#39;, assume_sorted=True,
            bounds_error=False, fill_value=fill_value)

    def cdf(self, k):
        # We do not need the default argument checking from scipy
        # because we handle the invalid values differently using
        # the class `interp1d_withwarning`. Therefore:
        k = np.asarray(k)
        return self._cdf(k)

    def ppf(self, q):
        # We do not need the default argument checking from scipy
        # because we handle the invalid values differently using
        # the class `interp1d_withwarning`. Therefore:
        q = np.asarray(q)
        if np.any(q &lt; 0) or np.any(q &gt; 1):
            raise ValueError(&#39;Some values out of ppf support [0, 1].&#39;)
        return self._ppf(q)

    def _get_pdf(self):
        &#34;&#34;&#34;
        Interpolates a derivative of cdf to obtain the pdf.
        &#34;&#34;&#34;
        g = np.linspace(*self._get_support(), self.bins)
        c = self.cdf(g)  # Evaluated cdf
        dx = 1 / (g[1] - g[0])
        dif = np.round(np.ediff1d(c, to_begin=c[1] - c[0]) * dx, decimals=10)
        return interp1d_with_warning(g, dif, kind=&#39;linear&#39;, assume_sorted=True)

    def _entropy(self, *args):
        &#34;&#34;&#34;
        Differential entropy of the learned RV.
        &#34;&#34;&#34;
        from scipy.special import entr
        from scipy.integrate import simpson
        g = np.linspace(*self._get_support(), self.bins)
        return simpson(entr(self._pdf(g)), x=g)

    def rvs(self, size, random_state=None):
        &#34;&#34;&#34;
        Random sample from the learned distribution.
        &#34;&#34;&#34;
        if random_state is None or type(random_state) is int:
            from numpy.random import default_rng
            random_state = default_rng(random_state)
        return self.ppf(
            random_state.uniform(*self._get_support_ppf(), size=size))


def save_redistributor(d, path):
    &#34;&#34;&#34;Saves the Redistributor or Redistributor_multi object to a file.&#34;&#34;&#34;
    import joblib
    joblib.dump(d, path)


def load_redistributor(path):
    &#34;&#34;&#34;Loads the Redistributor or Redistributor_multi object from a file.&#34;&#34;&#34;
    import joblib
    return joblib.load(path)


def plot_cdf_ppf_pdf(dist, a=None, b=None, bins=None,
                     v=None, w=None, rows=1, cols=3, 
                     figsize=(16, 5)):
    &#34;&#34;&#34;
    Just a convinience function for visualizing the dist
    `cdf`, `ppf` and `pdf` functions.
    
    Parameters
    ----------
    a: float
        Start of the cdf support
    b: float
        End of the cdf support
    v: float
        Start of the ppf support
    w: float
        End of the ppf support
    rows: int, 
        Number of rows in the figure
    cols: int, 
        Number of cols in the figure
    figsize: None or tuple
        If None, no new figure is created.
    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    if a is None:
        a = dist._get_support()[0]
    if b is None:
        b = dist._get_support()[1]
    if bins is None:
        bins = dist.bins

    if v is None:
        v = dist._get_support_ppf()[0]
    if w is None:
        w = dist._get_support_ppf()[1]

    x = np.linspace(a, b, bins)
    y = np.linspace(v, w, bins)

    if figsize is not None:
        plt.figure(figsize=figsize)
        plt.tight_layout()
        
    plt.subplot(rows, cols, 1)
    plt.title(f&#39;{dist.name} CDF&#39;)
    plt.plot(x, dist.cdf(x))
    

    plt.subplot(rows, cols, 2)
    plt.title(f&#39;{dist.name} PPF&#39;)
    plt.plot(y, dist.ppf(y))
    

    plt.subplot(rows, cols, 3)
    plt.title(f&#39;{dist.name} PDF&#39;)
    plt.plot(x, dist.pdf(x))
    plt.ylim(-0.001, None)
    
    if figsize is not None:
        plt.show()
        plt.close()
        return 
    else:    
        return plt


class interp1d_with_warning(interp1d):
    &#34;&#34;&#34;
    By default behaves exactly as scipy.interpolate.interp1d but allows
    the user to specify `bounds_error = &#39;warn&#39;` which overrides the
    behaviour of `_check_bunds` to warn instead of raising an error.

    Parameters
    ----------
    Accepts all the args and kwargs as scipy.interpolate.interp1d.
    &#34;&#34;&#34;

    def __init__(self, *args, **kwargs):
        self.warn = False
        bounds_error = kwargs.get(&#39;bounds_error&#39;)
        if bounds_error == &#39;warn&#39;:
            self.warn = True
            bounds_error = True
        super().__init__(*args, **kwargs)

    def _check_bounds(self, x_new):
        &#34;&#34;&#34;
        Overriding the _check_bounds method of scipy.interpolate.interp1d
        in order to provide a functionality of warning the user instead of
        just throwing an error when some value is out of bounds. Even if
        fill_value is specified a warning can be issued to let the user
        know it was necessary to use the fill_value and from which side.
        &#34;&#34;&#34;

        below_bounds = x_new &lt; self.x[0]  # Find values which are bellow bounds
        above_bounds = x_new &gt; self.x[-1]  # Find values which are above bounds

        msg = (&#34;{} out of {} values in x_new are {} the interpolation &#34;
               &#34;range. Read the docs of `fill_value` and `bounds_error` &#34;
               &#34;to manage the behavior.&#34;)

        if below_bounds.any():
            if self.bounds_error:
                m = msg.format(below_bounds.sum(), below_bounds.size, &#39;below&#39;)
                if self.warn:
                    m += (&#39; Mapping the invalid values to value: &#39;
                          f&#39;{self._fill_value_below}.&#39;)
                    warnings.warn(m)
                else:
                    raise ValueError(m)

        if above_bounds.any():
            if self.bounds_error:
                m = msg.format(above_bounds.sum(), above_bounds.size, &#39;above&#39;)
                if self.warn:
                    m += (&#39; Mapping the invalid values to value: &#39;
                          f&#39;{self._fill_value_above}.&#39;)
                    warnings.warn(m)
                else:
                    raise ValueError(m)

        return below_bounds, above_bounds


def make_unique(array, random_state, mode=&#39;spread&#39;, duplicates=None):
    &#34;&#34;&#34;
    Takes a sorted array and adjusts the duplicate values such that all
    elements of the array are unique. The adjustment is done by linearly
    separating the duplicates. Read more in docsting of `_get_intervals`.

    In case `mode=&#39;keep&#39;` this function does nothing and returns the array.

    Supports two deterministic modes &#39;spread&#39; and &#39;cluster&#39;. These two
    define onto how large interval the valueas are spread. If &#39;cluster&#39;
    is not possible &#39;spread&#39; is used implicitly.

    In case there are too many duplicates (&gt;5e3), first uses addition of
    random noise to non-min and non-max values and then continues with the
    deterministic method.

    Keeps the min, max, and unique values unchanged.
    If the first iteration did not make all elements unique, repeats until
    failure and warns the user (should be rare).


    Parameters
    ----------
    array: 1D numpy array
        Sorted array with potential of having non-unique elements.

    random_state: RandomState

    mode: str, one of {&#39;keep&#39;, &#39;spread&#39;, &#39;cluster&#39;, &#39;noise&#39;}
        Cluster adjusts the values by a tiny amount only.
        Spread uses all available space between consecutive vals.

    duplicates: int, number of duplicates in previous iteration.
        Do not use, used only for recursion.


    Returns
    --------
    Sorted array of unique elements on the orignal interval.
    &#34;&#34;&#34;

    if mode == &#39;keep&#39;:
        return array

    def _get_intervals(array, diff):
        &#34;&#34;&#34;
        Iterates over the diff of the array, when it finds a duplicate
        value (i.e., when diff == 0), it adds it to the result dict and
        finds the interval onto which the duplicates can be spread such
        that the value does not jump over previous/next value or it&#39;s
        intrval. If two duplicate values are right after each other, they
        share the interval between them based on number of duplicates each
        of tham has. Note that with array [2, 2] the number of duplicates
        of the value 2 is counted as 1. The other is original.

        Returns
        -------
        dict {value: [n: int, n of val duplicates,
                      i: int, first index of duplicate value,
                      j: int, last index of duplicate value,
                      a: int, start of safe interval,
                      b: int, end of safe interval]}

        Example
        -------

        array = [0,0,1,3,4,6,6,6,7,7,9,9]
        diff  = [0,1,2,1,2,0,0,1,0,2,0,9]
        r = {
            0: [1, 0, 1, 0, 1],
            6: [2, 5, 7, 4, 6.666666666666667],
            7: [1, 8, 9, 6.666666666666667, 7.923076923076923],
            9: [1, 10, 11, 7.923076923076923, 9]}
        &#34;&#34;&#34;
        r = {}
        n = 0
        prev = v = i = j = a = b = None
        for p, (d, dd) in enumerate(zip(diff, diff[1:])):
            if d == 0:  # Duplicate value
                v = array[p]
                if n == 0:  # First occurence
                    a = array[p] if p == 0 else array[p - 1]
                    i = p  # First occurence index
                n += 1
                if dd != 0:  # Last occurence
                    b = array[p + 2] if p + 2 &lt; array.size else array[p + 1]
                    j = p + 1  # Last occurence index

                    # Two duplicates next to each other must share interval
                    # Proportion for each is assigned based on their counts
                    if prev is not None and v == r[prev][4]:
                        pn = r[prev][0]  # Previous n
                        d1 = prev - r[prev][3]  # Interval left of prev
                        d2 = v - prev  # Interval left of current value
                        d3 = b - v  # Interval right of current value
                        pw = (pn * d2) / (d1 + d2)  # weight of prev val
                        cw = (n * d2) / (d2 + d3)  # weight of current val
                        a = prev + (d2 * pw) / (pw + cw)
                        r[prev][4] = a  # Adjust previous val&#39;s b value

                    # Store result and restart counters
                    r[v] = [n, i, j, a, b]  # Mutable for adjustments
                    prev = v
                    v = i = j = a = b = None
                    n = 0
        return r

    eps = 1e3 * np.finfo(array.dtype).eps

    # Assuming sorted array
    _min, _max = array[0], array[-1]
    diff = np.ediff1d(array, to_end=_max)
    if np.any(diff &lt; 0):
        raise ValueError(&#39;Array must be sorted.&#39;)

    # Find all duplicates
    dupl = diff == 0

    # No work if no duplicates
    n_duplicates = dupl.sum()
    if n_duplicates == 0:
        return array

    if n_duplicates == duplicates:
        warnings.warn((
            f&#39;Returning non-unique. Unable to remove {n_duplicates} &#39;
            f&#39;({(n_duplicates / array.size) * 100}%) duplicates.&#39;))
        return array

    warnings.warn(
        (f&#39;Adjusting {n_duplicates / array.size * 100}% non-unique &#39;
         &#39;lattice values. Avoid learning discrete distributions.&#39;))

    if (n_duplicates &gt; int(5e3) or mode == &#39;noise&#39;) and duplicates is None:
        warnings.warn((
            f&#39;Array has too many duplicates ({n_duplicates}) &#39;
            &#39;to use the deterministic algorithm. Solving some &#39;
            &#39;or all by adding small random noise.&#39;))
        change = dupl &amp; np.logical_not((array == _min) | (array == _max))
        array[change] += random_state.uniform(eps, 10 * eps, change.sum())
        return make_unique(
            np.sort(array), random_state, mode, n_duplicates)
    else:
        # If there is not that many duplicates, we use
        # this method, which would be otherwise slower.
        intervals = _get_intervals(array, diff)
        for k, v in intervals.items():
            n, i, j, a, b = v
            if mode == &#39;cluster&#39;:
                a = np.max([a, k - n * eps])
                b = np.min([b, k + n * eps])

            # Using size n+1+n%2 to avoid a and k in lin
            lin = np.linspace(a, b, n + 1 + n % 2, endpoint=False,
                              dtype=array.dtype)[1 + n % 2:]
            if k in lin:  # Accidentely linspace falls on k
                lin = np.sort(np.random.uniform(a, b, n))
            assert a not in lin and k not in lin, (
                &#39;a or k in lin, this is a bug. Pls report. &#39;
                f&#39; {k}, {n}, {a}, {b}, {lin}&#39;)
            array[i:j] = lin

        return make_unique(
            np.sort(array), random_state, mode, n_duplicates)
    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="redistributor.load_redistributor"><code class="name flex">
<span>def <span class="ident">load_redistributor</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the Redistributor or Redistributor_multi object from a file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_redistributor(path):
    &#34;&#34;&#34;Loads the Redistributor or Redistributor_multi object from a file.&#34;&#34;&#34;
    import joblib
    return joblib.load(path)</code></pre>
</details>
</dd>
<dt id="redistributor.make_unique"><code class="name flex">
<span>def <span class="ident">make_unique</span></span>(<span>array, random_state, mode='spread', duplicates=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes a sorted array and adjusts the duplicate values such that all
elements of the array are unique. The adjustment is done by linearly
separating the duplicates. Read more in docsting of <code>_get_intervals</code>.</p>
<p>In case <code>mode='keep'</code> this function does nothing and returns the array.</p>
<p>Supports two deterministic modes 'spread' and 'cluster'. These two
define onto how large interval the valueas are spread. If 'cluster'
is not possible 'spread' is used implicitly.</p>
<p>In case there are too many duplicates (&gt;5e3), first uses addition of
random noise to non-min and non-max values and then continues with the
deterministic method.</p>
<p>Keeps the min, max, and unique values unchanged.
If the first iteration did not make all elements unique, repeats until
failure and warns the user (should be rare).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>array</code></strong> :&ensp;<code>1D numpy array</code></dt>
<dd>Sorted array with potential of having non-unique elements.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>RandomState</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str, one</code> of <code>{'keep', 'spread', 'cluster', 'noise'}</code></dt>
<dd>Cluster adjusts the values by a tiny amount only.
Spread uses all available space between consecutive vals.</dd>
</dl>
<p>duplicates: int, number of duplicates in previous iteration.
Do not use, used only for recursion.</p>
<h2 id="returns">Returns</h2>
<p>Sorted array of unique elements on the orignal interval.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_unique(array, random_state, mode=&#39;spread&#39;, duplicates=None):
    &#34;&#34;&#34;
    Takes a sorted array and adjusts the duplicate values such that all
    elements of the array are unique. The adjustment is done by linearly
    separating the duplicates. Read more in docsting of `_get_intervals`.

    In case `mode=&#39;keep&#39;` this function does nothing and returns the array.

    Supports two deterministic modes &#39;spread&#39; and &#39;cluster&#39;. These two
    define onto how large interval the valueas are spread. If &#39;cluster&#39;
    is not possible &#39;spread&#39; is used implicitly.

    In case there are too many duplicates (&gt;5e3), first uses addition of
    random noise to non-min and non-max values and then continues with the
    deterministic method.

    Keeps the min, max, and unique values unchanged.
    If the first iteration did not make all elements unique, repeats until
    failure and warns the user (should be rare).


    Parameters
    ----------
    array: 1D numpy array
        Sorted array with potential of having non-unique elements.

    random_state: RandomState

    mode: str, one of {&#39;keep&#39;, &#39;spread&#39;, &#39;cluster&#39;, &#39;noise&#39;}
        Cluster adjusts the values by a tiny amount only.
        Spread uses all available space between consecutive vals.

    duplicates: int, number of duplicates in previous iteration.
        Do not use, used only for recursion.


    Returns
    --------
    Sorted array of unique elements on the orignal interval.
    &#34;&#34;&#34;

    if mode == &#39;keep&#39;:
        return array

    def _get_intervals(array, diff):
        &#34;&#34;&#34;
        Iterates over the diff of the array, when it finds a duplicate
        value (i.e., when diff == 0), it adds it to the result dict and
        finds the interval onto which the duplicates can be spread such
        that the value does not jump over previous/next value or it&#39;s
        intrval. If two duplicate values are right after each other, they
        share the interval between them based on number of duplicates each
        of tham has. Note that with array [2, 2] the number of duplicates
        of the value 2 is counted as 1. The other is original.

        Returns
        -------
        dict {value: [n: int, n of val duplicates,
                      i: int, first index of duplicate value,
                      j: int, last index of duplicate value,
                      a: int, start of safe interval,
                      b: int, end of safe interval]}

        Example
        -------

        array = [0,0,1,3,4,6,6,6,7,7,9,9]
        diff  = [0,1,2,1,2,0,0,1,0,2,0,9]
        r = {
            0: [1, 0, 1, 0, 1],
            6: [2, 5, 7, 4, 6.666666666666667],
            7: [1, 8, 9, 6.666666666666667, 7.923076923076923],
            9: [1, 10, 11, 7.923076923076923, 9]}
        &#34;&#34;&#34;
        r = {}
        n = 0
        prev = v = i = j = a = b = None
        for p, (d, dd) in enumerate(zip(diff, diff[1:])):
            if d == 0:  # Duplicate value
                v = array[p]
                if n == 0:  # First occurence
                    a = array[p] if p == 0 else array[p - 1]
                    i = p  # First occurence index
                n += 1
                if dd != 0:  # Last occurence
                    b = array[p + 2] if p + 2 &lt; array.size else array[p + 1]
                    j = p + 1  # Last occurence index

                    # Two duplicates next to each other must share interval
                    # Proportion for each is assigned based on their counts
                    if prev is not None and v == r[prev][4]:
                        pn = r[prev][0]  # Previous n
                        d1 = prev - r[prev][3]  # Interval left of prev
                        d2 = v - prev  # Interval left of current value
                        d3 = b - v  # Interval right of current value
                        pw = (pn * d2) / (d1 + d2)  # weight of prev val
                        cw = (n * d2) / (d2 + d3)  # weight of current val
                        a = prev + (d2 * pw) / (pw + cw)
                        r[prev][4] = a  # Adjust previous val&#39;s b value

                    # Store result and restart counters
                    r[v] = [n, i, j, a, b]  # Mutable for adjustments
                    prev = v
                    v = i = j = a = b = None
                    n = 0
        return r

    eps = 1e3 * np.finfo(array.dtype).eps

    # Assuming sorted array
    _min, _max = array[0], array[-1]
    diff = np.ediff1d(array, to_end=_max)
    if np.any(diff &lt; 0):
        raise ValueError(&#39;Array must be sorted.&#39;)

    # Find all duplicates
    dupl = diff == 0

    # No work if no duplicates
    n_duplicates = dupl.sum()
    if n_duplicates == 0:
        return array

    if n_duplicates == duplicates:
        warnings.warn((
            f&#39;Returning non-unique. Unable to remove {n_duplicates} &#39;
            f&#39;({(n_duplicates / array.size) * 100}%) duplicates.&#39;))
        return array

    warnings.warn(
        (f&#39;Adjusting {n_duplicates / array.size * 100}% non-unique &#39;
         &#39;lattice values. Avoid learning discrete distributions.&#39;))

    if (n_duplicates &gt; int(5e3) or mode == &#39;noise&#39;) and duplicates is None:
        warnings.warn((
            f&#39;Array has too many duplicates ({n_duplicates}) &#39;
            &#39;to use the deterministic algorithm. Solving some &#39;
            &#39;or all by adding small random noise.&#39;))
        change = dupl &amp; np.logical_not((array == _min) | (array == _max))
        array[change] += random_state.uniform(eps, 10 * eps, change.sum())
        return make_unique(
            np.sort(array), random_state, mode, n_duplicates)
    else:
        # If there is not that many duplicates, we use
        # this method, which would be otherwise slower.
        intervals = _get_intervals(array, diff)
        for k, v in intervals.items():
            n, i, j, a, b = v
            if mode == &#39;cluster&#39;:
                a = np.max([a, k - n * eps])
                b = np.min([b, k + n * eps])

            # Using size n+1+n%2 to avoid a and k in lin
            lin = np.linspace(a, b, n + 1 + n % 2, endpoint=False,
                              dtype=array.dtype)[1 + n % 2:]
            if k in lin:  # Accidentely linspace falls on k
                lin = np.sort(np.random.uniform(a, b, n))
            assert a not in lin and k not in lin, (
                &#39;a or k in lin, this is a bug. Pls report. &#39;
                f&#39; {k}, {n}, {a}, {b}, {lin}&#39;)
            array[i:j] = lin

        return make_unique(
            np.sort(array), random_state, mode, n_duplicates)</code></pre>
</details>
</dd>
<dt id="redistributor.plot_cdf_ppf_pdf"><code class="name flex">
<span>def <span class="ident">plot_cdf_ppf_pdf</span></span>(<span>dist, a=None, b=None, bins=None, v=None, w=None, rows=1, cols=3, figsize=(16, 5))</span>
</code></dt>
<dd>
<div class="desc"><p>Just a convinience function for visualizing the dist
<code>cdf</code>, <code>ppf</code> and <code>pdf</code> functions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>float</code></dt>
<dd>Start of the cdf support</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>float</code></dt>
<dd>End of the cdf support</dd>
<dt><strong><code>v</code></strong> :&ensp;<code>float</code></dt>
<dd>Start of the ppf support</dd>
<dt><strong><code>w</code></strong> :&ensp;<code>float</code></dt>
<dd>End of the ppf support</dd>
<dt><strong><code>rows</code></strong> :&ensp;<code>int, </code></dt>
<dd>Number of rows in the figure</dd>
<dt><strong><code>cols</code></strong> :&ensp;<code>int, </code></dt>
<dd>Number of cols in the figure</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>None</code> or <code>tuple</code></dt>
<dd>If None, no new figure is created.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_cdf_ppf_pdf(dist, a=None, b=None, bins=None,
                     v=None, w=None, rows=1, cols=3, 
                     figsize=(16, 5)):
    &#34;&#34;&#34;
    Just a convinience function for visualizing the dist
    `cdf`, `ppf` and `pdf` functions.
    
    Parameters
    ----------
    a: float
        Start of the cdf support
    b: float
        End of the cdf support
    v: float
        Start of the ppf support
    w: float
        End of the ppf support
    rows: int, 
        Number of rows in the figure
    cols: int, 
        Number of cols in the figure
    figsize: None or tuple
        If None, no new figure is created.
    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    if a is None:
        a = dist._get_support()[0]
    if b is None:
        b = dist._get_support()[1]
    if bins is None:
        bins = dist.bins

    if v is None:
        v = dist._get_support_ppf()[0]
    if w is None:
        w = dist._get_support_ppf()[1]

    x = np.linspace(a, b, bins)
    y = np.linspace(v, w, bins)

    if figsize is not None:
        plt.figure(figsize=figsize)
        plt.tight_layout()
        
    plt.subplot(rows, cols, 1)
    plt.title(f&#39;{dist.name} CDF&#39;)
    plt.plot(x, dist.cdf(x))
    

    plt.subplot(rows, cols, 2)
    plt.title(f&#39;{dist.name} PPF&#39;)
    plt.plot(y, dist.ppf(y))
    

    plt.subplot(rows, cols, 3)
    plt.title(f&#39;{dist.name} PDF&#39;)
    plt.plot(x, dist.pdf(x))
    plt.ylim(-0.001, None)
    
    if figsize is not None:
        plt.show()
        plt.close()
        return 
    else:    
        return plt</code></pre>
</details>
</dd>
<dt id="redistributor.save_redistributor"><code class="name flex">
<span>def <span class="ident">save_redistributor</span></span>(<span>d, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the Redistributor or Redistributor_multi object to a file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_redistributor(d, path):
    &#34;&#34;&#34;Saves the Redistributor or Redistributor_multi object to a file.&#34;&#34;&#34;
    import joblib
    joblib.dump(d, path)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="redistributor.LearnedDistribution"><code class="flex name class">
<span>class <span class="ident">LearnedDistribution</span></span>
<span>(</span><span>x, a=None, b=None, bins=None, keep_x_unchanged=True, subsample_x=None, ravel_x=True, assume_sorted=False, fill_value='auto', bounds_error='warn', dupl_method='spread', seed=None, name='LearnedDistribution', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A continuous random variable obtained by estimating the empirical
distribution of a user provided 1D array of numeric data <code>x</code>. It
can be used to sample new random points from the learned distribution.</p>
<p>It approximates the Cumulative Distribution Function (<code>cdf</code>) and
Percent Point Function (<code>ppf</code>) of the underlying distribution of <code>x</code>
using linear interpolation on a lattice.</p>
<p>An approximation of the Probability Density Function (<code>pdf</code>) is
computed as an interpolation of the numerical derivative of the <code>cdf</code>
function. Please note it can oscilate a lot if <code>bins</code> is high.</p>
<p>The distribution is defined on a closed finite interval <code>[a, b]</code> or
<code>[xmin, xmax]</code> or combination thereof, depending on which bound/s
were specified by the user.</p>
<p>WARNING: It can not be used to learn discrete distributions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D numpy array</code></dt>
<dd>1D vector of which the distribution will be estimated.</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>numeric</code> or <code>None</code></dt>
<dd>Left boundary of the distribution support if known.
If specified, must be smaller than x.min().</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>numeric</code> or <code>None</code></dt>
<dd>Right boundary of the distribution support if known.
If specified, must be bigger than x.max().</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>User specified value of bins. Min is 3, max is <code>x.size</code>.
If None or 0, bins are set automatically. Upper bound
is set to 1000 to prevent unnecessary computation.
Used to specify the density of the lattice. More bins
means higher precision but also more computation.</dd>
<dt><strong><code>keep_x_unchanged</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>
<p>If True, the <code>x</code> array will be copied before partial sorting.
This will result in increased memory usage. But it will
not reorder the user provided array.</p>
<p>If False, there will not be any additional memory consumption.
But the user provided array <code>x</code> might change its order.
This might be very useful if <code>x</code> is a large array and there is
not enough available memory.</p>
</dd>
<dt><strong><code>subsample_x</code></strong> :&ensp;<code>int</code>, default <code>None</code></dt>
<dd>Sacrifice precision for speed by first subsampling array <code>x</code>
with a defined integer step. Not doing <code>random.choice()</code> but rather
simple <code>slice(None, None, subsample_x)</code> because it is faster and
we assume the array is randomly ordered. Can lead to significant
speedups.</dd>
<dt><strong><code>ravel_x</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>LearnedDistribution requires 1D arrays. So the <code>x</code> is by default
flattened to 1D using <code>np.ravel()</code>.</dd>
<dt><strong><code>assume_sorted</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If the user knows that <code>x</code> is sorted, setting this to True will
save a most of time by ommiting partial sorting the array.
Especially useful if the array <code>x</code> is big. E.g. 1GB of data
takes approx. 10s to partial sort on 5000 positions.
If <code>False</code> and <code>x</code> is almost sorted, it will still be faster than
if <code>x</code> is randomly ordered.</dd>
<dt><strong><code>fill_value</code></strong> :&ensp;<code>None, array-like, float, 2-tuple</code> or <code>'auto'</code>, default=<code>'auto'</code></dt>
<dd>Specifies where to map the values out of the <code>cdf</code> support. See the
docstring of scipy.interpolate.interp1d to learn more about the
valid options. Additionally, this class enables the user to use
the default <code>auto</code> option, which sets reasonable <code>fill_value</code>
automatically.</dd>
<dt><strong><code>bounds_error</code></strong> :&ensp;<code>bool</code> or <code>'warn'</code>, default <code>'warn'</code></dt>
<dd>See the docstring of class interp1d_with_warning.</dd>
<dt><strong><code>dupl_method</code></strong> :&ensp;<code>str, one</code> of <code>{'keep', 'spread', 'cluster', 'noise'}</code></dt>
<dd>default 'spread'
Method of solving duplicate lattice values. Read more in
docstring of <code><a title="redistributor.make_unique" href="#redistributor.make_unique">make_unique()</a></code>.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, default <code>'LearnedDistribution'</code></dt>
<dd>The name of the instance.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>{None, int,</code>numpy.random.Generator<code>,</code></dt>
<dd><code>numpy.random.RandomState</code>}, default None
See the docstring of scipy.stats.rv_continuous.
Used in <code>_prevent_same()</code> and <code>rvs()</code>.</dd>
</dl>
<p>kwargs : all other keyword arguments accepted by rv_continous.</p>
<h2 id="methods-todo-finish-this-documentation">Methods - TODO finish this documentation</h2>
<p>cdf
ppf
pdf
rvs
entropy
&hellip; fill in the rest which is implemented
&hellip; handle the rest which does not make sense</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LearnedDistribution(rv_continuous):
    &#34;&#34;&#34;
        A continuous random variable obtained by estimating the empirical
        distribution of a user provided 1D array of numeric data `x`. It
        can be used to sample new random points from the learned distribution.

        It approximates the Cumulative Distribution Function (`cdf`) and
        Percent Point Function (`ppf`) of the underlying distribution of `x`
        using linear interpolation on a lattice.

        An approximation of the Probability Density Function (`pdf`) is
        computed as an interpolation of the numerical derivative of the `cdf`
        function. Please note it can oscilate a lot if `bins` is high.

        The distribution is defined on a closed finite interval `[a, b]` or
        `[xmin, xmax]` or combination thereof, depending on which bound/s
        were specified by the user.

        WARNING: It can not be used to learn discrete distributions.


        Parameters
        ----------

        x : 1D numpy array
            1D vector of which the distribution will be estimated.

        a : numeric or None
            Left boundary of the distribution support if known.
            If specified, must be smaller than x.min().

        b : numeric or None
            Right boundary of the distribution support if known.
            If specified, must be bigger than x.max().

        bins : int or None
            User specified value of bins. Min is 3, max is `x.size`.
            If None or 0, bins are set automatically. Upper bound
            is set to 1000 to prevent unnecessary computation.
            Used to specify the density of the lattice. More bins
            means higher precision but also more computation.

        keep_x_unchanged : bool, default True
            If True, the `x` array will be copied before partial sorting.
            This will result in increased memory usage. But it will
            not reorder the user provided array.

            If False, there will not be any additional memory consumption.
            But the user provided array `x` might change its order.
            This might be very useful if `x` is a large array and there is
            not enough available memory.

        subsample_x : int, default None
            Sacrifice precision for speed by first subsampling array `x`
            with a defined integer step. Not doing `random.choice()` but rather
            simple `slice(None, None, subsample_x)` because it is faster and
            we assume the array is randomly ordered. Can lead to significant
            speedups.

        ravel_x : bool, default True
            LearnedDistribution requires 1D arrays. So the `x` is by default
            flattened to 1D using `np.ravel()`.

        assume_sorted : bool, default False
            If the user knows that `x` is sorted, setting this to True will
            save a most of time by ommiting partial sorting the array.
            Especially useful if the array `x` is big. E.g. 1GB of data
            takes approx. 10s to partial sort on 5000 positions.
            If `False` and `x` is almost sorted, it will still be faster than
            if `x` is randomly ordered.

        fill_value : None, array-like, float, 2-tuple or &#39;auto&#39;, default=&#39;auto&#39;
            Specifies where to map the values out of the `cdf` support. See the
            docstring of scipy.interpolate.interp1d to learn more about the
            valid options. Additionally, this class enables the user to use
            the default `auto` option, which sets reasonable `fill_value`
            automatically.

        bounds_error : bool or &#39;warn&#39;, default &#39;warn&#39;
            See the docstring of class interp1d_with_warning.

        dupl_method : str, one of {&#39;keep&#39;, &#39;spread&#39;, &#39;cluster&#39;, &#39;noise&#39;}
                      default &#39;spread&#39;
            Method of solving duplicate lattice values. Read more in
            docstring of `make_unique()`.

        name : str, default &#39;LearnedDistribution&#39;
            The name of the instance.

        seed : {None, int, `numpy.random.Generator`,
            `numpy.random.RandomState`}, default None
            See the docstring of scipy.stats.rv_continuous.
            Used in `_prevent_same()` and `rvs()`.

        kwargs : all other keyword arguments accepted by rv_continous.


        Methods - TODO finish this documentation
        -------

        cdf
        ppf
        pdf
        rvs
        entropy
        ... fill in the rest which is implemented
        ... handle the rest which does not make sense
        &#34;&#34;&#34;

    def __init__(self, x, a=None, b=None, bins=None, keep_x_unchanged=True,
                 subsample_x=None, ravel_x=True, assume_sorted=False,
                 fill_value=&#39;auto&#39;, bounds_error=&#39;warn&#39;, dupl_method=&#39;spread&#39;,
                 seed=None, name=&#39;LearnedDistribution&#39;, **kwargs):

        super().__init__(name=name, seed=seed, **kwargs)

        if ravel_x:
            x = x.ravel()

        # Sacrifice precision for speed
        if isinstance(subsample_x, int):
            if 2 &lt;= subsample_x &lt;= x.size:
                x = x[::subsample_x]
            else:
                raise ValueError(&#39;Not 2 &lt;= subsample_x &lt;= x.size.&#39;)

        # Handling input data and interval
        self._validate_x(x)
        self.xmin = x.min()
        self.xmax = x.max()
        self._validate_a_b(a, b)
        self.a = a
        self.b = b

        # Arguments for interpolation
        self.bounds_error = bounds_error
        self.fill_value = fill_value

        # Setting lattice density
        self.bins = self._infer_bins(x.size, bins)

        # Interpolating to get the empirical distribution
        self.assume_sorted = assume_sorted
        self.dupl_method = dupl_method
        lattice, vals = self._get_lattice_and_vals(x, keep_x_unchanged)
        self._cdf = self._get_cdf(lattice, vals)
        self._ppf = self._get_ppf(lattice, vals)
        self._pdf = self._get_pdf()

    def _get_support(self, *args):
        &#34;&#34;&#34;
        Support of LearnedDistribution does not depend on any scipy arguments,
        we keep args only to keep the signature unchanged.

        The support depends only on whether `a` and/or `b` were specified
        explicitely or as Nones.

        `self.a` and/or `self.b` are kept stored as Nones to keep the
        information about the object config for future reference of the user.

        Returns
        -------
        a, b : numeric (float, or int)
            end-points of the distribution&#39;s support.
        &#34;&#34;&#34;
        return self._cdf.x[0], self._cdf.x[-1]

    def _get_support_ppf(self, *args):
        &#34;&#34;&#34;
        The support of `ppf` in scipy is always `[0,1]` so this method does not
        exist in `rv_continuous`. In our case, the support might be shrinked if
        any of the a, b is set to None.
        &#34;&#34;&#34;
        return self._ppf.x[0], self._ppf.x[-1]

    def _validate_x(self, data):
        &#34;&#34;&#34;
        Validation of the input data.

        Parameters
        --------
        data: numpy array of data to be validated
        &#34;&#34;&#34;
        if not np.issubdtype(data.dtype, np.floating):
            raise TypeError(&#39;Input array dtype must be floating point.&#39;)
        if np.issubdtype(data.dtype, np.float16):
            warnings.warn((
                &#39;Using float16 data can lead to large errors. &#39;
                &#39;Rather use f32 or f64.&#39;))
        if not data.ndim == 1:
            raise ValueError(&#39;Input array must be 1D. You can use x.ravel().&#39;)

    def _validate_a_b(self, a, b):
        &#34;&#34;&#34;
        Validation of the boundaries.

        Parameters
        --------
        a: numeric or None
            See the docstring of __init__().
        b: numerc or None
            See the docstring of __init__().
        &#34;&#34;&#34;
        if a is not None:
            if not np.isfinite(a):
                raise ValueError(f&#39;a {a} must be finite.&#39;)
            if a &gt;= self.xmin:
                raise ValueError(f&#39;a {a} must be &lt; than xmin {self.xmin}.&#39;)
        if b is not None:
            if not np.isfinite(b):
                raise ValueError(f&#39;b {b} must be finite.&#39;)
            if b &lt;= self.xmax:
                raise ValueError(f&#39;b {b} must be &gt; than xmax {self.xmax}.&#39;)

    def _infer_bins(self, n, bins):
        &#34;&#34;&#34;
        Infers and validates the number of bins.

        Parameters
        --------
        n: int
            Size of the data.

        bins: int or None
            See the docstring of __init__().
        &#34;&#34;&#34;
        if bins is None or bins == 0:
            bins = min(n, int(5e3))
        if bins &gt; n or bins &lt; 3:
            raise ValueError(f&#39;Bins ({bins}) must be 2 &lt; bins &lt;= x.size&#39;)

        return bins
    

    def _get_lattice_and_vals(self, x, keep_x_unchanged):
        &#34;&#34;&#34;
        Creating the `lattice` based on the number of `bins` and assembling the
        corresp. `lattice_vals` from provided array `x` using partial sort.

        Parameters
        --------
        x: 1D numpy array
            See the docstring of __init__().

        keep_x_unchanged: bool
            See the docstring of __init__().

        Returns
        -------

        lattice, 1D array of equidistant values
            Support of ppf or range of cdf. The first value of the array will
            be either 0 or epsilon and the last value will be either 1 or
            1 - epsilon depending if a or b are specified or None. Size or the
            array will range from bins to bins + 2.

        lattice_vals, 1D array
            Range of ppf or support of cdf. The first value of the array will
            be either xmin or a and the last value will be either xmax or b
            depending if a or b are specified or None. Size of the array will
            be the same as of `lattice`.
        &#34;&#34;&#34;

        # Do we need expansion by a or b from Left or Right side or both?
        L, R = self.a is not None, self.b is not None

        # Indices at which we need x to be sorted considering L and R
        indices = np.linspace(
            0, x.size + L + R - 1, self.bins).round().astype(int)
        indices = indices[L:-1 if R else None] - L

        if not self.assume_sorted:
            # Reorder values of x on indices as if the array was sorted
            if keep_x_unchanged:
                # Does not change x but uses twice the memory
                x = np.partition(x, indices)
            else:
                # Does not use additional memory but alters the order of data
                x.partition(indices)

        # Get the _ppf.y (or _cdf.x) values
        eps = 1 / (x.size + 1)  # Shrink the lattice with eps to exclude 0 or 1
        lattice_vals = np.hstack([[self.a] * L, x[indices], [self.b] * R])

        # Get the _ppf.x (or _cdf.y) values
        lattice = np.linspace(
            eps * (not L), 1 - eps * (not R), lattice_vals.size)

        if not all(np.isfinite(lattice_vals)):
            raise ValueError(&#39;Values of x on the lattice must be finite.&#39;)

        # If necessary, make duplicate values unique and warn the user.
        lattice_vals = make_unique(
            lattice_vals, self.random_state, mode=self.dupl_method)
        return lattice, lattice_vals

    def _get_cdf(self, lattice, lattice_vals):
        &#34;&#34;&#34;
        Interpolates the lattice on lattice_vals to get the `cdf`.

        Table of `fill_value` if `self.fill_value == &#39;auto&#39;` (n = x.size):
        ------------------------------------------------------------------
        a     | b     | cdf support | truncated to | fill_value
        ------------------------------------------------------------------
        None  | None  | xmin, xmax  | xmin, xmax   | 1/(n-1), (n-2)/(n-1)
        None  | b     | xmin, b     | xmin, None   | 1/(n-1), None
        a     | None  | a,    xmax  | None, xmax   | None, (n-2)/(n-1)
        a     | b     | a,    b     | None, None   | None, None
        &#34;&#34;&#34;

        if self.fill_value == &#39;auto&#39;:
            fill_value = (lattice[0] if self.a is None else None,
                          lattice[-1] if self.b is None else None)
        else:
            # User defined fill_value
            fill_value = self.fill_value

        return interp1d_with_warning(
            lattice_vals, lattice, kind=&#39;linear&#39;, assume_sorted=True,
            bounds_error=self.bounds_error, fill_value=fill_value)

    def _get_ppf(self, lattice, lattice_vals):
        &#34;&#34;&#34;
        Interpolates the lattice_vals on a lattice to get the `ppf`.
        `fill_values` is set to the `xmin` and `xmax` to avoid problems
        when generating random sample with `rvs()`. `bounds_error` is
        set to `False` because user does not need a warning about this
        behaviour and values `q &lt; 0` or `q &gt; 1` should not ever occur at all.
        &#34;&#34;&#34;
        fill_value = (lattice_vals[0], lattice_vals[-1])
        return interp1d_with_warning(
            lattice, lattice_vals, kind=&#39;linear&#39;, assume_sorted=True,
            bounds_error=False, fill_value=fill_value)

    def cdf(self, k):
        # We do not need the default argument checking from scipy
        # because we handle the invalid values differently using
        # the class `interp1d_withwarning`. Therefore:
        k = np.asarray(k)
        return self._cdf(k)

    def ppf(self, q):
        # We do not need the default argument checking from scipy
        # because we handle the invalid values differently using
        # the class `interp1d_withwarning`. Therefore:
        q = np.asarray(q)
        if np.any(q &lt; 0) or np.any(q &gt; 1):
            raise ValueError(&#39;Some values out of ppf support [0, 1].&#39;)
        return self._ppf(q)

    def _get_pdf(self):
        &#34;&#34;&#34;
        Interpolates a derivative of cdf to obtain the pdf.
        &#34;&#34;&#34;
        g = np.linspace(*self._get_support(), self.bins)
        c = self.cdf(g)  # Evaluated cdf
        dx = 1 / (g[1] - g[0])
        dif = np.round(np.ediff1d(c, to_begin=c[1] - c[0]) * dx, decimals=10)
        return interp1d_with_warning(g, dif, kind=&#39;linear&#39;, assume_sorted=True)

    def _entropy(self, *args):
        &#34;&#34;&#34;
        Differential entropy of the learned RV.
        &#34;&#34;&#34;
        from scipy.special import entr
        from scipy.integrate import simpson
        g = np.linspace(*self._get_support(), self.bins)
        return simpson(entr(self._pdf(g)), x=g)

    def rvs(self, size, random_state=None):
        &#34;&#34;&#34;
        Random sample from the learned distribution.
        &#34;&#34;&#34;
        if random_state is None or type(random_state) is int:
            from numpy.random import default_rng
            random_state = default_rng(random_state)
        return self.ppf(
            random_state.uniform(*self._get_support_ppf(), size=size))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scipy.stats._distn_infrastructure.rv_continuous</li>
<li>scipy.stats._distn_infrastructure.rv_generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="redistributor.LearnedDistribution.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>self, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Cumulative distribution function of the given RV.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array_like</code></dt>
<dd>quantiles</dd>
<dt>arg1, arg2, arg3,&hellip; : array_like</dt>
<dt>The shape parameter(s) for the distribution (see docstring of the</dt>
<dt>instance object for more information)</dt>
<dt><strong><code>loc</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>location parameter (default=0)</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>scale parameter (default=1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cdf</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Cumulative distribution function evaluated at <code>x</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdf(self, k):
    # We do not need the default argument checking from scipy
    # because we handle the invalid values differently using
    # the class `interp1d_withwarning`. Therefore:
    k = np.asarray(k)
    return self._cdf(k)</code></pre>
</details>
</dd>
<dt id="redistributor.LearnedDistribution.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q)</span>
</code></dt>
<dd>
<div class="desc"><p>Percent point function (inverse of <code>cdf</code>) at q of the given RV.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>array_like</code></dt>
<dd>lower tail probability</dd>
<dt>arg1, arg2, arg3,&hellip; : array_like</dt>
<dt>The shape parameter(s) for the distribution (see docstring of the</dt>
<dt>instance object for more information)</dt>
<dt><strong><code>loc</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>location parameter (default=0)</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>scale parameter (default=1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array_like</code></dt>
<dd>quantile corresponding to the lower tail probability q.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppf(self, q):
    # We do not need the default argument checking from scipy
    # because we handle the invalid values differently using
    # the class `interp1d_withwarning`. Therefore:
    q = np.asarray(q)
    if np.any(q &lt; 0) or np.any(q &gt; 1):
        raise ValueError(&#39;Some values out of ppf support [0, 1].&#39;)
    return self._ppf(q)</code></pre>
</details>
</dd>
<dt id="redistributor.LearnedDistribution.rvs"><code class="name flex">
<span>def <span class="ident">rvs</span></span>(<span>self, size, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Random sample from the learned distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rvs(self, size, random_state=None):
    &#34;&#34;&#34;
    Random sample from the learned distribution.
    &#34;&#34;&#34;
    if random_state is None or type(random_state) is int:
        from numpy.random import default_rng
        random_state = default_rng(random_state)
    return self.ppf(
        random_state.uniform(*self._get_support_ppf(), size=size))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="redistributor.Redistributor"><code class="flex name class">
<span>class <span class="ident">Redistributor</span></span>
<span>(</span><span>source, target)</span>
</code></dt>
<dd>
<div class="desc"><p>An algorithm for automatic transformation of data from arbitrary
distribution into arbitrary distribution. Source and target distributions
can be known beforehandand or learned from the data using
LearnedDistribution class. Transformation is piecewise linear, monotonic
and invertible.</p>
<p>Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
and saved to be used later for transforming other data assuming the same
source distribution.</p>
<p>Uses source's and target's <code>cdf()</code> and <code>ppf()</code> to infer the
transform and inverse transform functions.</p>
<p><code>transform_function = target_ppf(source_cdf(x))</code>
<code>inverse_transform = source_ppf(target_cdf(x))</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Redistributor(TransformerMixin):
    &#34;&#34;&#34;
    An algorithm for automatic transformation of data from arbitrary
    distribution into arbitrary distribution. Source and target distributions
    can be known beforehandand or learned from the data using
    LearnedDistribution class. Transformation is piecewise linear, monotonic
    and invertible.

    Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
    and saved to be used later for transforming other data assuming the same
    source distribution.

    Uses source&#39;s and target&#39;s `cdf()` and `ppf()` to infer the
    transform and inverse transform functions.

    `transform_function = target_ppf(source_cdf(x))`
    `inverse_transform = source_ppf(target_cdf(x))`
    &#34;&#34;&#34;

    def __init__(self, source, target):
        self.source = source
        self.target = target

    def fit(x=None, y=None):
        &#34;&#34;&#34;
        Redistributor does not need to be fitted.
        &#34;&#34;&#34;
        pass

    def transform(self, x):
        &#34;&#34;&#34;
        Transform the data from source to target distribution.
        &#34;&#34;&#34;
        return self.target.ppf(self.source.cdf(x))

    def inverse_transform(self, x):
        &#34;&#34;&#34;
        Inverse transform the data from target to source distribution.
        &#34;&#34;&#34;
        return self.source.ppf(self.target.cdf(x))

    def kstest(self, n=20):
        &#34;&#34;&#34;
        Performs the (one-sample or two-sample) Kolmogorov-Smirnov test.
        &#34;&#34;&#34;
        from scipy.stats import kstest
        return kstest(self.source.rvs, self.target.cdf, N=n,
                      alternative=&#39;two-sided&#39;, mode=&#39;auto&#39;)

    def plot_transform_function(self, bins=1000, newfig=True, figsize=(16, 5)):
        &#34;&#34;&#34;
        Plotting the learned transformation from source to target.
        &#34;&#34;&#34;
        import matplotlib.pyplot as plt
        x = np.linspace(*self.source._get_support(), bins)
        t = self.transform(x)
        if newfig:
            plt.figure(figsize=figsize)
        plt.title(&#39;Transform function&#39;)
        plt.plot(x, t)
        if newfig:
            plt.show()
            plt.close()
        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="redistributor.Redistributor.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>x=None, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Redistributor does not need to be fitted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(x=None, y=None):
    &#34;&#34;&#34;
    Redistributor does not need to be fitted.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse transform the data from target to source distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, x):
    &#34;&#34;&#34;
    Inverse transform the data from target to source distribution.
    &#34;&#34;&#34;
    return self.source.ppf(self.target.cdf(x))</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.kstest"><code class="name flex">
<span>def <span class="ident">kstest</span></span>(<span>self, n=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the (one-sample or two-sample) Kolmogorov-Smirnov test.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kstest(self, n=20):
    &#34;&#34;&#34;
    Performs the (one-sample or two-sample) Kolmogorov-Smirnov test.
    &#34;&#34;&#34;
    from scipy.stats import kstest
    return kstest(self.source.rvs, self.target.cdf, N=n,
                  alternative=&#39;two-sided&#39;, mode=&#39;auto&#39;)</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.plot_transform_function"><code class="name flex">
<span>def <span class="ident">plot_transform_function</span></span>(<span>self, bins=1000, newfig=True, figsize=(16, 5))</span>
</code></dt>
<dd>
<div class="desc"><p>Plotting the learned transformation from source to target.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_transform_function(self, bins=1000, newfig=True, figsize=(16, 5)):
    &#34;&#34;&#34;
    Plotting the learned transformation from source to target.
    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    x = np.linspace(*self.source._get_support(), bins)
    t = self.transform(x)
    if newfig:
        plt.figure(figsize=figsize)
    plt.title(&#39;Transform function&#39;)
    plt.plot(x, t)
    if newfig:
        plt.show()
        plt.close()
    return</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform the data from source to target distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x):
    &#34;&#34;&#34;
    Transform the data from source to target distribution.
    &#34;&#34;&#34;
    return self.target.ppf(self.source.cdf(x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="redistributor.interp1d_with_warning"><code class="flex name class">
<span>class <span class="ident">interp1d_with_warning</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>By default behaves exactly as scipy.interpolate.interp1d but allows
the user to specify <code>bounds_error = 'warn'</code> which overrides the
behaviour of <code>_check_bunds</code> to warn instead of raising an error.</p>
<h2 id="parameters">Parameters</h2>
<p>Accepts all the args and kwargs as scipy.interpolate.interp1d.</p>
<p>Initialize a 1-D linear interpolation class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class interp1d_with_warning(interp1d):
    &#34;&#34;&#34;
    By default behaves exactly as scipy.interpolate.interp1d but allows
    the user to specify `bounds_error = &#39;warn&#39;` which overrides the
    behaviour of `_check_bunds` to warn instead of raising an error.

    Parameters
    ----------
    Accepts all the args and kwargs as scipy.interpolate.interp1d.
    &#34;&#34;&#34;

    def __init__(self, *args, **kwargs):
        self.warn = False
        bounds_error = kwargs.get(&#39;bounds_error&#39;)
        if bounds_error == &#39;warn&#39;:
            self.warn = True
            bounds_error = True
        super().__init__(*args, **kwargs)

    def _check_bounds(self, x_new):
        &#34;&#34;&#34;
        Overriding the _check_bounds method of scipy.interpolate.interp1d
        in order to provide a functionality of warning the user instead of
        just throwing an error when some value is out of bounds. Even if
        fill_value is specified a warning can be issued to let the user
        know it was necessary to use the fill_value and from which side.
        &#34;&#34;&#34;

        below_bounds = x_new &lt; self.x[0]  # Find values which are bellow bounds
        above_bounds = x_new &gt; self.x[-1]  # Find values which are above bounds

        msg = (&#34;{} out of {} values in x_new are {} the interpolation &#34;
               &#34;range. Read the docs of `fill_value` and `bounds_error` &#34;
               &#34;to manage the behavior.&#34;)

        if below_bounds.any():
            if self.bounds_error:
                m = msg.format(below_bounds.sum(), below_bounds.size, &#39;below&#39;)
                if self.warn:
                    m += (&#39; Mapping the invalid values to value: &#39;
                          f&#39;{self._fill_value_below}.&#39;)
                    warnings.warn(m)
                else:
                    raise ValueError(m)

        if above_bounds.any():
            if self.bounds_error:
                m = msg.format(above_bounds.sum(), above_bounds.size, &#39;above&#39;)
                if self.warn:
                    m += (&#39; Mapping the invalid values to value: &#39;
                          f&#39;{self._fill_value_above}.&#39;)
                    warnings.warn(m)
                else:
                    raise ValueError(m)

        return below_bounds, above_bounds</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scipy.interpolate.interpolate.interp1d</li>
<li>scipy.interpolate.polyint._Interpolator1D</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#compatibility">Compatibility</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#mathematical-description">Mathematical description</a></li>
<li><a href="#how-to-cite">How to cite</a></li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgement">Acknowledgement</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="redistributor.load_redistributor" href="#redistributor.load_redistributor">load_redistributor</a></code></li>
<li><code><a title="redistributor.make_unique" href="#redistributor.make_unique">make_unique</a></code></li>
<li><code><a title="redistributor.plot_cdf_ppf_pdf" href="#redistributor.plot_cdf_ppf_pdf">plot_cdf_ppf_pdf</a></code></li>
<li><code><a title="redistributor.save_redistributor" href="#redistributor.save_redistributor">save_redistributor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="redistributor.LearnedDistribution" href="#redistributor.LearnedDistribution">LearnedDistribution</a></code></h4>
<ul class="">
<li><code><a title="redistributor.LearnedDistribution.cdf" href="#redistributor.LearnedDistribution.cdf">cdf</a></code></li>
<li><code><a title="redistributor.LearnedDistribution.ppf" href="#redistributor.LearnedDistribution.ppf">ppf</a></code></li>
<li><code><a title="redistributor.LearnedDistribution.rvs" href="#redistributor.LearnedDistribution.rvs">rvs</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code></h4>
<ul class="">
<li><code><a title="redistributor.Redistributor.fit" href="#redistributor.Redistributor.fit">fit</a></code></li>
<li><code><a title="redistributor.Redistributor.inverse_transform" href="#redistributor.Redistributor.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="redistributor.Redistributor.kstest" href="#redistributor.Redistributor.kstest">kstest</a></code></li>
<li><code><a title="redistributor.Redistributor.plot_transform_function" href="#redistributor.Redistributor.plot_transform_function">plot_transform_function</a></code></li>
<li><code><a title="redistributor.Redistributor.transform" href="#redistributor.Redistributor.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="redistributor.interp1d_with_warning" href="#redistributor.interp1d_with_warning">interp1d_with_warning</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>