<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>redistributor API documentation</title>
<meta name="description" content="**Redistributor** is a tool for automatic transformation of empirical data distributions. It is implemented in **Python3** as a **Scikit-learn â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>redistributor</code></h1>
</header>
<section id="section-intro">
<p><strong>Redistributor</strong> is a tool for automatic transformation of empirical data distributions. It is implemented in <strong>Python3</strong> as a <strong>Scikit-learn transformer</strong>.</p>
<p>It allows the user to transform their data from arbitrary distribution into other arbitrary distribution. The source and target distributions can be specified exactly, if known beforehand, or can be inferred from the data. Transformation is <strong>piece-wise smooth, monotonic, and invertible</strong>, and can be <strong>saved for later use</strong> on different data assuming the same source distribution.</p>
<p>The empirical distribution can be inferred from a 1D array of data. To redistribute multiple slices of your data use <code><a title="redistributor.Redistributor_multi" href="#redistributor.Redistributor_multi">Redistributor_multi</a></code> class which has a <strong>low memory footprint</strong> and utilizes <strong>parallel computing</strong> to apply multiple <code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code> objects.</p>
<h2 id="installation">Installation</h2>
<table>
<thead>
<tr>
<th align="center">:warning:</th>
<th align="left">Not yet published on PyPi. Coming soon.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The code is hosted in this <a href="https://gitlab.com/paloha/redistributor">GitLab repository</a>.
To install the released version from Pypi use:</p>
<pre><code class="language-bash">pip install redistributor
</code></pre>
<p>Or install the bleeding edge directly from git:</p>
<pre><code class="language-bash">pip install git+https://gitlab.com/paloha/redistributor
</code></pre>
<p>For development, install the package in editable mode with extra dependencies for documentation and testing:</p>
<pre><code class="language-bash"># Clone the repository
git clone git@gitlab.com:paloha/redistributor.git
cd redistributor

 # Use virtual environment [optional]
python3 -m virtualenv .venv
source .venv/bin/activate

# Install with pip in editable mode
pip install -e .[dev]
</code></pre>
<h2 id="compatibility">Compatibility</h2>
<h2 id="dependencies">Dependencies</h2>
<p>Required packages for <code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code> are specified in the <code>install_requires</code> list in the <code>setup.py</code> file.</p>
<p>Extra dependencies for running the tests, compiling the documentation, or running the examples are specified in the <code>extras_require</code> dictionary in the same file.</p>
<p>The full version-locked list of dependencies and subdependencies is frozen in <code>requirements.txt</code>. Installing with <code>pip install -r requirements.txt</code> in a virtual environment should always lead to a fully functional project.</p>
<h2 id="mathematical-description">Mathematical description</h2>
<p>Assume we are given data <span><span class="MathJax_Preview">x\sim S</span><script type="math/tex">x\sim S</script></span> distributed according to some source distribution <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> on <span><span class="MathJax_Preview">\mathbb{R}</span><script type="math/tex">\mathbb{R}</script></span> and our goal is to find a transformation <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> such that <span><span class="MathJax_Preview">R(x)\sim T</span><script type="math/tex">R(x)\sim T</script></span> for some target distribution <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> on <span><span class="MathJax_Preview">\mathbb{R}</span><script type="math/tex">\mathbb{R}</script></span>.</p>
<p>One can mathematically show that a suitable <span><span class="MathJax_Preview">R\colon \mathbb{R} \to \mathbb{R}</span><script type="math/tex">R\colon \mathbb{R} \to \mathbb{R}</script></span> is given by
<span><span class="MathJax_Preview">
R := F_{T}^{-1} \circ F_{S},
</span><script type="math/tex; mode=display">
R := F_{T}^{-1} \circ F_{S},
</script></span>
where <span><span class="MathJax_Preview">F_S</span><script type="math/tex">F_S</script></span> and <span><span class="MathJax_Preview">F_T</span><script type="math/tex">F_T</script></span> are the cumulative distribution functions of <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> and <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>, respectively.</p>
<p>If <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> and <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is unknown, one can use approximations <span><span class="MathJax_Preview">\tilde{F}_S</span><script type="math/tex">\tilde{F}_S</script></span> and <span><span class="MathJax_Preview">\tilde{F}_T</span><script type="math/tex">\tilde{F}_T</script></span> of the corresponding cumulative distribution functions given by interpolating (partially) sorted data
<span><span class="MathJax_Preview">
(x_i)_{i=1}^N \ \text{with} \ x_i \sim S
</span><script type="math/tex; mode=display">
(x_i)_{i=1}^N \ \text{with} \ x_i \sim S
</script></span>
<span><span class="MathJax_Preview">
(y_i)_{i=1}^M \ \text{with} \ y_i \sim T.
</span><script type="math/tex; mode=display">
(y_i)_{i=1}^M \ \text{with} \ y_i \sim T.
</script></span>
Defining
<span><span class="MathJax_Preview">
\tilde{R} := \tilde{F}_{T}^{-1} \circ \tilde{F}_S,
</span><script type="math/tex; mode=display">
\tilde{R} := \tilde{F}_{T}^{-1} \circ \tilde{F}_S,
</script></span>
one can, under suitable conditions, show that
<span><span class="MathJax_Preview">
\tilde{R} \xrightarrow[N,M\to \infty]{} R.
</span><script type="math/tex; mode=display">
\tilde{R} \xrightarrow[N,M\to \infty]{} R.
</script></span></p>
<h2 id="how-to-cite">How to cite</h2>
<h2 id="license">License</h2>
<p>This project is licensed under the terms of the MIT license.
See <code>license.txt</code> for details.</p>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>This work was supported by the <em>International Mobility of Researchers</em> (program call no.: <a href="https://opvvv.msmt.cz/vyzva/vyzva-c-02-16-027-mezinarodni-mobilita-vyzkumnych-pracovniku.htm">CZ.02.2.69/0.0/0.0/16027/0008371</a>).
<img alt="opvvv" src="https://gitlab.com/paloha/redistributor/uploads/19903a1b9e00015faa2b61234a99b911/opvvv.jpg"></p>
<h2 id="to-do-list">To do list</h2>
<p>:white_check_mark: use code checker (flake8 + autopep8)
:black_square_button: specify compatibility with Python versions in this readme
:black_square_button: specify compatibility with operating systems in this readme
:black_square_button: add citation bibtex in this readme
:black_square_button: update urls in setup.py
:black_square_button: update acknowledgement in this readme
:black_square_button: use <a href="https://tox.readthedocs.io/en/latest/example/pytest.html">pytest and tox</a> for testing
:black_square_button: publish on PyPi <a href="https://packaging.python.org/guides/distributing-packages-using-setuptools">guide here</a>
:black_square_button: verify integrity of html and pdf outputs + gitlab and gitlab pages display</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
.. include:: readme.md
&#34;&#34;&#34;

from __future__ import division

import sys
import itertools

import numpy as np
from scipy.stats import norm
from sklearn.base import TransformerMixin


def save_redistributor(d, path):
    &#34;&#34;&#34;Saves the Redistributor or Redistributor_multi object to a file.&#34;&#34;&#34;
    import joblib
    joblib.dump(d, path)


def load_redistributor(path):
    &#34;&#34;&#34;Loads the Redistributor or Redistributor_multi object from a file.&#34;&#34;&#34;
    import joblib
    return joblib.load(path)


def _divisors(x, returnOne=True, returnX=True):
    &#34;&#34;&#34;
    Generates all divisors of a number. Divisors are not yielded in order.
    By default, 1 and x are included in result.

    Parameters
    --------
    x: int
        Number for which we need to find all its divisors.

    returnOne: bool, optional, default True
        Flag specifying if number 1 should be included in results.

    returnX: bool, optional, default True
        Flag specifying if the number x itself should be included in results.

    Yields
    --------
    One int at a time, that divides x equally. No duplicates.
    &#34;&#34;&#34;
    assert x &gt; 0, &#39;Must be positive &gt; 0&#39;
    if x == 1:
        if returnOne or returnX:
            yield 1
            return
        else:
            return
    if returnOne:
        yield 1
    if returnX:
        yield x
    i = 2
    while i * i &lt;= x:
        if x % i == 0:
            yield i
            if x // i != i:
                yield x // i
        i += 1


class Redistributor(TransformerMixin):
    &#34;&#34;&#34;
    An algorithm for automatic transformation of data from arbitrary
    distribution into arbitrary distribution. Source distribution
    can be known beforehandand or learned from the data. Transformation
    is piecewise smooth, monotonic and invertible.

    Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
    (for more dimensions use Redistributor_multi wrapper) and saved to be used
    later for transforming other data assuming the same source distribution.

    Uses source&#39;s and target&#39;s cdf() and ppf() to infer the
    transform and inverse transform functions.

    transform_function = target_ppf(source_cdf(x))
    inverse_transform = source_ppf(target_cdf(x))

    Time complexity
    --------
    This algorithm can scale to a large number of samples &gt; 1e08.
    Algorithm can be sped up by:
    1. Turning off input data validation by setting validate_input=False
    2. Choosing target distribution with fast cdf and ppf functions.
    3. Specifying the source distribution if it is known.

    Parameters
    --------
    target: obj, optional, default scipy.stats.norm(loc=0, scale=1)
        Class specifying the target distribution. Must implement
        cdf(), ppf() and ideally pdf() methods.
        Continuous distributions from scipy.stats can be used.

    known_distribution: obj, optional, default None
        Object specifying the source distribution when known.
        Must implement cdf(), ppf() and ideally pdf() methods.
        Continuous distributions from scipy.stats can be used.

    bbox: tuple, optional, default None
        Tuple (a, b) which represent the lower and upper boundary
        of the source distribution&#39;s domain.

    closed_interval: bool, optional, default True
        If True, the values set in bounding box are included into
        the interpolation range.

    prevent_same: bool, optional, default True
        Flag, specifyig whether to prevent same values in the
        interpolated vector.

    tinity: int, default 1e06
        Specifies divisor of noise added to vectors with same elements.
        Bigger the tinity, smaller the noise.

    validate_input: bool, optional, default True
        Flag specifying if the input data should be validated.
        Turn off to save computation time. Output is undefined
        if the data are not valid.

    bins: int, optional, default 0
        Number of bins which are used to infer the latice density
        when the distribution is learned. Irelevant with known_distribution.

        If bins = x.size, then latice density = 100%. The interpolation step =
        x.size // bins = 1. This is most precise. If x.size is big enough,
        we can lower the latice density and the fit will remain very precise.

        If bins = 0, it is set automatically. The latice density = 100%
        is then used up to x.size = 5000. If x.size &gt; 5000, bins = 5000.

    Attributes
    --------
    self.target_cdf: callable, default scipy.stats.norm(loc=0, scale=1).cdf
        Cummulative Density Function of target distribution.

    self.target_ppf: callable, default scipy.stats.norm(loc=0, scale=1).ppf
        Percent Point Function (inverse of cdf) of target distribution.

    self.source_cdf: callable, default None
        Either known_distribution.cdf function of source distribution
        or learned on fit() by interpolation on latice.

    self.source_ppf: callable, default None
        Either known_distribution.ppf function of source distribution
        or learned on fit() by interpolation on latice.

    self.a: float, default None
        Beginning of the interval of the source distribution domain.

    self.b: float, default None
        End of the interval of the source distribution domain.

    self.n: int, default None
        Size of the training data extened by bounding box borders if necessary.

    self.fitted: bool, default False
        Flag that specifies whether the fit() was already called.

    Limitations
    --------
    - Output is undefined when most of the data points have the exact
      same value, therefore data should not be sparse - multiple zeros etc.
      Handles small amounts of the exact same values by adding tiny noise.

    - Plotting of learned pdf() is just a smoothed approximation obtained
      by 1st derivative of the learned piece-wise smooth cdf() and it
      serves only as a visual aid.

    Examples
    --------
    TODO

    &#34;&#34;&#34;

    def __init__(self,
                 target=norm(loc=0, scale=1),
                 known_distribution=None,
                 bbox=None,
                 closed_interval=True,
                 prevent_same=True,
                 tinity=int(1e06),
                 validate_input=True,
                 bins=0):

        self.known_distribution = known_distribution
        self.bbox = bbox
        self.closed_interval = closed_interval
        self.prevent_same = prevent_same
        self.tinity = tinity
        self.validate_input = validate_input

        self.bins = bins  # Might chang on fit()
        self.target_cdf = target.cdf
        self.target_ppf = target.ppf

        # Will be computed or changed on fit()
        self.source_cdf = None
        self.source_ppf = None
        self.a = None
        self.b = None
        self.n = None
        self.fitted = False

        if self.known_distribution is not None:
            self.fit()

    def fit(self, x=None):
        &#34;&#34;&#34;
        Calls all necessary methods to infer cdf and ppf of the source
        distribution. Source distribution can be either specified directly by
        its function or learned from the training data on a closed or opened
        interval. Learning approximates the cdf and ppf by interpolating on a
        latice which density is infered from bins.

        Parameters
        --------

        x: 1D numpy array, optional, default None
            Training data from which the source distribution is learned.
            Must be specified if self.known_distribution is None.
        &#34;&#34;&#34;

        if self.known_distribution is not None:
            assert self.bbox is not None, \
                &#39;Bounding box must be specified when using known_distrubition.&#39;
            self._infer_a_b(x, self.bbox)
            try:
                self.source_cdf = self.known_distribution.cdf
                self.source_ppf = self.known_distribution.ppf
            except AttributeError:
                print(&#39;&#39;&#39;Class known_distribution must implement cdf(), ppf()
                and ideally pdf() methods.&#39;&#39;&#39;, file=sys.stderr)
                raise
            try:
                self.source_pdf = self.known_distribution.pdf
            except AttributeError:
                self.source_pdf = None
            self.known_distribution = self.known_distribution
        else:
            assert x is not None, \
                &#39;If known_distribution is None, X must be specified.&#39;

            # Calculate bounding box
            self._infer_a_b(x, self.bbox)

            # Validate input data
            if self.validate_input:
                self._validate_input(x)

            # Compute number of bins to use
            self._infer_nbins(x.size, self.bins)

            # Ensure, that a and b values are in x
            x = self._enforce_borders(x, self.closed_interval)

            # Get the size of x with borders a and b
            self.n = x.size

            # Learn the source distribution
            self._infer_cdf_ppf(x, self.prevent_same)

        self.fitted = True
        return self

    def transform(self, x):
        &#34;&#34;&#34;
        Applies learned transformation function to the data x.

        Parameters
        --------
        x : numpy array
            Data to transform. Must be within self.a, self.b interval.

        Returns
        --------
        Numpy array of transformed data.
        &#34;&#34;&#34;

        if self.validate_input:
            self._validate_input(x)
        return self.target_ppf(self.source_cdf(x))

    def inverse_transform(self, x):
        &#34;&#34;&#34;
        Applies learned inverse transform function to the data x.

        Parameters
        --------
        x : numpy array
            Data to inverse transform. Must be within the interp. interval.
            of learned transform function.

        Returns
        --------
        Numpy array of inverse transformed data.
        &#34;&#34;&#34;

        return self.source_ppf(self.target_cdf(x))

    def _validate_input(self, data):
        &#34;&#34;&#34;
        Validation of the input data used to validate inputs to fit() and
        transform(). Can be turned off globally by setting
        self.validate_input = False.

        Parameters
        --------
        data: numpy array of data to be validated
        &#34;&#34;&#34;
        assert data.dtype == np.float64 or data.dtype == np.float32, \
            &#39;Data must be float64 of 32.&#39;
        assert data.ndim == 1, \
            &#39;Data must be stored in 1D numpy array. You can use data.ravel().&#39;
        assert all(np.isfinite(data)), \
            &#39;Data must not contain np.nan or np.inf.&#39;
        assert self.a &lt;= np.min(data) and np.max(data) &lt;= self.b, \
            &#39;Data out of interval set by bbox.&#39;

    def _infer_a_b(self, data, bbox):
        &#34;&#34;&#34;
        Infers self.a and self.b which represent the lower and upper boundary
        of the source distribution&#39;s domain.

        Parameters
        --------
        data: numpy array or None
            Training data used to infer the a and b automaticaly by taking min
            and max. Can&#39;t be None if the bbox is None.

        bbox: tuple or None
            Tuple (a, b) which represent the lower and upper boundary
            of the source distribution&#39;s domain.
        &#34;&#34;&#34;

        if bbox is None:
            bbox = (np.min(data), np.max(data))
        assert len(bbox) == 2, \
            &#39;Please specify just two values in bbox.&#39;
        assert bbox[0] &lt; bbox[1], \
            &#39;First element of bbox must be smaller than the second.&#39;
        self.a = bbox[0]
        self.b = bbox[1]

    def _enforce_borders(self, x, closed_interval):
        &#34;&#34;&#34;
        Inserts self.a and self.b into the array x on the first and last
        positions respectively. If closed_interval=True, the values are
        extended by small amount, so the actual values a and b are still
        included in the interpolation range.

        Parameters
        --------
        x: numpy array
            1D vector into which the values are inserted.

        closed_interval: bool
            Flag, specifying the opened or closed interval.
        &#34;&#34;&#34;
        xmin = np.min(x)
        xmax = np.max(x)
        extend = (xmax - xmin) / x.size / 100 if closed_interval else 0
        if xmin != self.a - extend:
            x = np.insert(x, 0, self.a - extend)
        if xmax != self.b + extend:
            x = np.append(x, self.b + extend)
        return x

    def _infer_nbins(self, n, bins):
        &#34;&#34;&#34;
        Infers or validates the number of bins.

        Parameters
        --------
        n: int
            Size of the training data.

        bins:
            User specified value of bins.
        &#34;&#34;&#34;
        if bins == 0:
            bins = min(n, 5000)
        assert bins &gt; 2 and bins &lt;= n
        self.bins = bins

    def _prevent_same(self, x):
        &#34;&#34;&#34;
        Adds tiny noise to prevent same values in a vector if there are any.

        Parameters
        --------
        x: 1D numpy array
            Array with potential of having not unique elements.

        Returns
        --------
        Sorted array with added small noise based on self.tinity. The aim is
        to have all elements in the array unique.
        &#34;&#34;&#34;
        s = np.array(list(set(x)))
        if x.size != len(s):

            # Find the smallest distance between two consecutive elements
            mindist = np.abs(np.min(np.ediff1d(s))) if len(s) &gt; 1 else 1

            # Add tiny noise that is smaller than the smalledst distance
            tiny_noise = np.random.rand(x.size) * (mindist / self.tinity)
            x += tiny_noise
            x = np.sort(x)

            # Recursion for handling extremely rare case that there
            # are still some elements that are not unique
            return self._prevent_same(x)
        else:
            return x

    def _infer_cdf_ppf(self, x, prevent_same):
        &#34;&#34;&#34;
        Learning of the target distribution is done by linear interpolation
        on a latice. This method infers approximation of source&#39;s
        Cumulative distribution function and Percent point function.

        Parameters
        --------
        x: numpy array
            1D vector used for the interpolation.
        prevent_same: bool
            Flag, specifying whether to add small noise to values on the
            latice so the interpolation data do not have the exact same values.
            Applied only if there are some.
        &#34;&#34;&#34;

        # Define latice on which to interpolate, more points = bigger precision
        latice = np.linspace(0, self.n - 1, self.bins).astype(int)

        # Get values of x on latice points by partial sort
        x.partition(latice)
        values_on_latice = np.sort(x[latice])

        # Adds tiny noise to prevent same values if necessary
        # Leave first and last values untouched to retain bbox
        if prevent_same:
            values_on_latice[1:-1] = self._prevent_same(values_on_latice[1:-1])

        # Normalize the latice by nubmer of samples
        # Cummulative density function must sum up to 1
        latice = latice / (self.n - 1)

        # Interpolate to get cdf and ppf
        from scipy.interpolate import interp1d
        self.source_cdf = interp1d(values_on_latice, latice, kind=&#39;linear&#39;)
        self.source_ppf = interp1d(latice, values_on_latice, kind=&#39;linear&#39;)

    def compute_empirical_cdf_error(self, x, error_func=&#39;mse&#39;):
        &#34;&#34;&#34;
        Computes error caused by approximation of transform function.
        Error computation needs to sort all data. It can take a long time
        for big arrays.

        Parameters
        --------
        x: numpy array
            1D vector of transformed data.
        error_func: callable or one of {&#39;mae&#39;, &#39;mse&#39;}
            Callable that computes error on two vectors.
            &#39;mae&#39; = Error in L1 norm (Mean Absolute Error)
            &#39;mse&#39; = Error in L2 norm (Mean Squared Error)

        Returns
        --------
        Float value of specified error or return value of callable.
        &#34;&#34;&#34;

        if error_func == &#39;mse&#39;:
            from sklearn.metrics import mean_squared_error as errfunc
        elif error_func == &#39;mae&#39;:
            from sklearn.metrics import mean_absolute_error as errfunc
        else:
            errfunc = error_func
            assert callable(errfunc), \
                &#39;Set error_func to &#34;mse&#34;, &#34;mae&#34; or func that computes error.&#39;

        transformed = self.transform(x)
        transformed = np.sort(transformed)
        n = transformed.size
        p_lower = self.target_cdf(transformed[0])
        p_upper = self.target_cdf(transformed[-1])
        v = p_lower + ((p_upper - p_lower) / (n - 1)) * np.arange(0, n)
        w = self.target_cdf(transformed)
        return errfunc(v, w)

    def plot_transform_function(self, figsize=(15, 2)):
        &#34;&#34;&#34;
        Displays matplotlib plot of the fitted transform function.

        Parameters
        --------
        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.
        &#34;&#34;&#34;
        assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

        x_axis = np.linspace(self.a, self.b, 1000)
        transformed = self.transform(x_axis)

        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        plt.plot(x_axis, transformed)
        plt.title(&#39;Learned monotonic piecewise smooth transform function&#39;)
        plt.xlim(self.a, self.b)
        plt.show()
        plt.close()

    def plot_source_pdf(self, smoothed=True, figsize=(15, 2)):
        &#34;&#34;&#34;
        Displays matplotlib plot of the learned source probability density
        function.

        Parameters
        --------
        smoothed: boolean, optional, default True
            Applies savgol_filter to smooth the curve that is disturbed by
            derivatives at bin transitions.

        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.
        &#34;&#34;&#34;
        assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

        steps = 1000
        step = (self.b - self.a) / steps

        x_axis = np.linspace(self.a, self.b, steps, endpoint=False)
        x_axis = x_axis[1:]

        if self.known_distribution:
            if self.source_pdf is not None:
                curve = self.source_pdf(x_axis)
                title = &#39;Pdf of known distribution.&#39;
                text = None
            else:
                print(&#39;&#39;&#39;Can&#39;t plot source pdf. Class of known distribution
                does not implement pdf().&#39;&#39;&#39;, file=sys.stderr)
                return
        else:
            from scipy.misc import derivative
            curve = derivative(self.source_cdf, x_axis, dx=step / 2)
            sm = &#39;&#39;
            if smoothed:
                from scipy.signal import savgol_filter
                win_width = steps // 16
                if win_width % 2 == 0:
                    win_width += 1
                curve = savgol_filter(curve, win_width, 1)
                sm = &#39; (smoothed)&#39;
            title = &#39;Source_pdf{} approximated using {} bins.&#39;.format(
                sm, self.bins)
            text = &#39;&#39;&#39;\nMay not show appropriate results with certain source
             distributions. In which case rather plot a histogram of your train
             data.&#39;&#39;&#39;

        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        plt.plot(x_axis, curve)
        plt.title(title)
        plt.xlim(self.a, self.b)
        plt.ylim((0., 1.2 * np.max(curve)))
        if text is not None:
            ax = plt.gca()
            plt.text(0.5, -0.4,
                     text,
                     size=10,
                     ha=&#39;center&#39;,
                     va=&#39;bottom&#39;,
                     transform=ax.transAxes)
        plt.show()
        plt.close()

    def plot_hist(self, data, nbins=None, title=&#39;Histogram&#39;,
                  figsize=(15, 2), xlim=None):
        &#34;&#34;&#34;
        Displays matplotlib histogram of the specified data and nbins.

        Parameters
        --------
        data : numpy array
            Specifying the position of data points on x axis.

        nbins : int, optional, default self.bins
            Specifying the number of bins of the histogram.
            If None, the number will be automatically set by matplotlib.

        title : str, optional, default &#39;Histogram&#39;
            Title of the plot.

        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.

        xlim : float, optional, default None
            Limit of x axis.
        &#34;&#34;&#34;
        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        if self.known_distribution is None:
            if nbins is None and self.n &gt; 2500:
                nbins = min(self.bins, 301)
        if xlim is not None:
            plt.xlim(xlim)
        plt.hist(data, nbins)
        plt.title(title)
        plt.show()
        plt.close()


class Redistributor_multi():
    &#34;&#34;&#34;
    Multi-dimensional wrapper for Redistributor.
    Allows to use multiple Redistributors on equal-sized slices (submatrices)
    of N-dimensional input array. Utilizes parallel processing with low memory
    footprint.

    Parameters:
    --------

    redistributors: numpy array of Redistributor objects
        Objects that will be used to operate on the data within each slice
        defined by nsub. Shape must be the same as nsub.

    nsub: tuple of int
        Tuple specifying how to split multidimensional input
        into submatrices. Corresponding redistributor object will
        be applied on each submatrix separately. Each int
        specifies to how many equal submatrices corresponding
        axis should be split. There must be exactly one int
        for each axis of multidimensional input data.

    cpus: int &gt;= 0
        Number of cpu cores to use in multiprocessing.
        If 0, all cores will be used.
    &#34;&#34;&#34;

    def __init__(self, redistributors, nsub, cpus=0):
        self.nsub = np.array(nsub)
        if np.prod(self.nsub == 1):
            print(&#39;&#39;&#39;WARNING: Using Redistributor_multi on whole matrix is
                slower than just Redistributor.&#39;&#39;&#39;, file=sys.stderr)
        self.redistributors = redistributors
        assert self.redistributors.size == np.prod(
            self.nsub), &#39;Specify one redistributor per slice.&#39;
        self.cpus = self._infer_cpus(cpus)
        self.fitted = False

    def _infer_cpus(self, cpus):
        &#34;&#34;&#34;
        Keeps the defined number of cpus or tries to set it to all
        available cpus if cpus = 0.
        &#34;&#34;&#34;
        if cpus == 0:
            try:
                from multiprocessing import cpu_count
                return cpu_count()
            except NotImplementedError:
                return 1  # default
        else:
            return cpus

    def fit(self, x=None, size_limit=0):
        &#34;&#34;&#34;
        Fits all Redistributor objects in self.redistributors.

        Parameters
        --------
        x : numpy array, optional, default None
            Data to fit on. If None, Redistributors must have
            source set explicitly.

        size_limit : float, optional, default 0
            Should be 3x smaller than available memory after loading the data
            in GB. Based on this value, the appropriate size of chunk is
            infered. If size_limit == 0, the best value is computed
            automatically.
        &#34;&#34;&#34;
        if x is not None:
            self.x = x
            self.machinery(&#39;fit&#39;, size_limit=size_limit)
        self.fitted = True

    def transform(self, x, inplace=True, size_limit=0):
        &#34;&#34;&#34;
        Transforms data in x using fitted Redistributors.

        Parameters
        --------
        x : numpy array
            Data to transform.

        inplace : bool, optional, default True
            Flag specifying whether to change the data in place without
            keeping the original values stored in x or operate on copy.

        size_limit : float, optional, default 0
            See docstring of self.fit

        Returns
        --------
        Transformed data with same shape as input.
        &#34;&#34;&#34;
        self.inplace = inplace
        self.x = x if self.inplace else x.copy()
        return self.machinery(&#39;transform&#39;, size_limit=size_limit)

    def inverse_transform(self, x, inplace=True, size_limit=0):
        &#34;&#34;&#34;
        Inverse transforms the data in x using fitted Redistributors.

        Parameters
        --------
        x : numpy array
            Data to transform.

        inplace : bool, optional, default True
            Flag specifying whether to change the data in place without
            keeping the original values stored in x or operate on copy.

        size_limit : float, optional, default 0
            See docstring of self.fit

        Returns
        --------
        Inverse transformed data with same shape as input.
        &#34;&#34;&#34;
        self.inplace = inplace
        self.x = x if self.inplace else x.copy()
        return self.machinery(&#39;inverse&#39;, size_limit=size_limit)

    def _locate_subarrays(self, xshape, nsub):
        &#34;&#34;&#34;
        Locates subarrays within matrix x according to nsub.

        Returns
        --------
        - Numpy array of indices that locate the subarrays within matrix x.
        - Shape of subarray that would be produced by slicing the matrix.
        Each subarray is of equal shape.
        &#34;&#34;&#34;

        nsub = np.array(nsub)
        shape = np.array(xshape)
        steps, rems = np.divmod(shape, nsub)
        assert all(rems == 0), \
            &#39;Nsub does not divide the x equally on {} axes.&#39;.format(
                np.nonzero(rems)[0])
        steps = np.array(shape / nsub).astype(int)
        output_shape = steps

        subarray_indices = [list(zip(
            range(0, shape[axis], steps[axis]),
            range(0 + steps[axis], shape[axis] + steps[axis], steps[axis])))
            for axis in range(len(nsub))]

        return (np.array(list(itertools.product(*subarray_indices))),
                output_shape)

    def _indices_to_slices(self, indices):
        &#34;&#34;&#34;
        Converts np array of start and stop indices to np array of slices.
        &#34;&#34;&#34;
        shape = indices.shape
        assert shape[-1] in (2, 3), \
            &#39;indices.shape[-1] must be 2 or 3 for (start, stop, [step]).&#39;
        indices = indices.reshape(-1, 2)
        return np.array([slice(*ind) for ind in indices]).reshape(shape[:-1])

    def _get_size_limit(self):
        &#34;&#34;&#34;
        Checks available memory and decides on size_limit for
        self._get_chunksize.
        &#34;&#34;&#34;
        try:
            import psutil
            size_limit = psutil.virtual_memory().available / 2.2e09
            if size_limit &lt; 0.1:
                print(&#39;&#39;&#39;WARNING: It seems you have too low available memory.
                The speed might be influenced significantly. For optimal speed
                it is good to have ~2x size_limit of free memory after loading
                tha data that are being processed. Size_limit for one chunk of
                sharedctypes array was set to default 0.5GB. Consider using
                self.cpus = 1.&#39;&#39;&#39;, file=sys.stderr)
                return 0.5
            else:
                return size_limit
        except:
            import sys
            print(&#39;&#39;&#39;WARNING: Unable to obtain the size of available memory.
            Size_limit for one chunk of sharedctypes array was set to default
            0.5GB. For optimal speed it is good to have ~2x size_limit of free
            memory after loading tha data that are being processed.&#39;&#39;&#39;,
                  file=sys.stderr)
            return 0.5

    def _get_chunksize(self, size_limit):
        &#34;&#34;&#34;
        Returns number of slices that should be in one chunk so the size of
        chunk is ideally equal to size_limit. Divides all slices to chunks
        with agreement to self.nsub. The size of chunk will be bigger if it is
        not possible. No matter the size, it never returns 1, because the whole
        class looses its meaning. Returns 1 only if the np.prod(self.nsub) == 1
        which is discouraged.

        Parameters
        --------
        size_limit : float
            Ideal size of one chunk so the sharedctypes array used
            in multiprocessing pipe in self.machinery is created as fast
            as possible.
        &#34;&#34;&#34;
        all_slices = np.prod(self.nsub)
        last_best = 1
        slice_size = self.x.nbytes / all_slices * 1e-09  # in GB
        for i, n in enumerate(self.nsub):
            if n == 1:
                continue
            returnOne = False if np.prod(self.nsub[i + 1:]) == 1 else True
            possible_steps_on_axis = np.array(list(reversed(sorted(_divisors(
                n, returnOne=returnOne, returnX=True)))))
            slices_in_chunk = possible_steps_on_axis * np.prod(
                self.nsub[i + 1:])
            for s in slices_in_chunk:
                if s * slice_size &lt;= size_limit:
                    return s
                last_best = s
        return last_best

    @staticmethod
    def init_global_array(array):
        &#34;&#34;&#34;Initializer of shared array for multiprocessing pool.&#34;&#34;&#34;
        global arr
        arr = array

    @staticmethod
    def populate(args):
        &#34;&#34;&#34;
        Static method called by child processes that applies desired
        function of redistributor on the data from shared matrix on
        desired location and populates the result back to the shared matrix.
        &#34;&#34;&#34;
        index, location, shape, redistributor, purpose, cpus = args
        if cpus == 1:
            # Get the access to the global array
            matrix = arr
        else:
            # Get the access to shraed array
            matrix = np.ctypeslib.as_array(arr)

        # Take the vector from the shared array
        v = matrix[tuple(location)].ravel()

        if purpose == &#39;fit&#39;:
            redistributor.fit(v)
        elif purpose == &#39;transform&#39;:
            matrix[tuple(location)] = redistributor.transform(v).reshape(shape)
        elif purpose == &#39;inverse&#39;:
            matrix[tuple(location)] = redistributor.inverse_transform(
                v).reshape(shape)

        return index, redistributor

    def machinery(self, purpose, size_limit):
        &#34;&#34;&#34;
        Handles locating subarrays and their parallel processing in chunks.

        Parameters
        --------
        purpose : one of {&#39;fit&#39;, &#39;transform&#39;, &#39;inverse&#39;}
            Specifies what should be done with the data.

        size_limit : float
            Ideal size of one chunk. 0 = automatic.
        &#34;&#34;&#34;

        # Number of all subarrays that will be used
        n_subarrays = np.prod(self.nsub)

        # Get subarray locations (list of indices)
        indices, output_shape = self._locate_subarrays(self.x.shape, self.nsub)

        # Avoiding multiprocessing and the overhead of creating shared_array
        if self.cpus == 1:
            locations = self._indices_to_slices(indices).tolist()
            Redistributor_multi.init_global_array(self.x)
            list(map(Redistributor_multi.populate,
                     zip(range(len(locations)),
                         locations,
                         itertools.repeat(output_shape),
                         self.redistributors.ravel(),
                         itertools.repeat(purpose),
                         itertools.repeat(self.cpus))))

        # Using pool of child processes running in parallel
        else:
            from multiprocessing import Pool
            from multiprocessing import RawArray

            # Get chunksize for splitting the self.x so creation of
            # shared_array is faster
            if size_limit == 0:
                size_limit = self._get_size_limit()
            chunksize = self._get_chunksize(size_limit)
            stepsize = n_subarrays // chunksize

            chunked_indices = indices.reshape(stepsize, -1, len(self.nsub), 2)
            locations_of_subarrays_within_each_chunk = self._indices_to_slices(
                chunked_indices[0]).tolist()

            for i, chunks_subarray_indices in enumerate(chunked_indices):
                # Find start and stop of chunk slice in each axis
                mins = np.min(chunks_subarray_indices, axis=2)[0]
                maxs = np.max(chunks_subarray_indices, axis=2)[-1]
                indices_of_chunk = np.array(list(zip(mins, maxs)))
                location_of_chunk = self._indices_to_slices(
                    indices_of_chunk).tolist()

                # Create a shared memory array that is accessible by the
                # child processes
                s = self.x[tuple(location_of_chunk)].copy()
                tmp = np.ctypeslib.as_ctypes(s)

                # Creating a shared array if much faster when the underlying
                # C code can make a copy of it. If there is not that much
                # available memory left it replaces value by value in place
                # which takes significantly more time. That is the reason this
                # is chunkized into smaller arrays and done in a for loop.
                shared_array = RawArray(tmp._type_, tmp)
                del s
                del tmp

                pool = Pool(processes=self.cpus,
                            initializer=Redistributor_multi.init_global_array,
                            initargs=(shared_array, ))

                p = pool.map(Redistributor_multi.populate,
                             zip(range(i * chunksize, (i + 1) * chunksize),
                                 locations_of_subarrays_within_each_chunk,
                                 itertools.repeat(output_shape),
                                 self.redistributors.ravel(),
                                 itertools.repeat(purpose),
                                 itertools.repeat(self.cpus)))

                # Update redistributor objects after being changed
                [np.put(self.redistributors, i, instance) for i, instance in p]

                pool.close()
                pool.join()

                if purpose != &#39;fit&#39;:
                    self.x[tuple(location_of_chunk)] = np.ctypeslib.as_array(
                        shared_array)

                # Freeing the memory
                del shared_array

        # Returning the results
        if purpose == &#39;fit&#39;:
            output = None
        else:
            output = self.x

        # Cleaning up the object
        del self.x
        return output</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="redistributor.load_redistributor"><code class="name flex">
<span>def <span class="ident">load_redistributor</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the Redistributor or Redistributor_multi object from a file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_redistributor(path):
    &#34;&#34;&#34;Loads the Redistributor or Redistributor_multi object from a file.&#34;&#34;&#34;
    import joblib
    return joblib.load(path)</code></pre>
</details>
</dd>
<dt id="redistributor.save_redistributor"><code class="name flex">
<span>def <span class="ident">save_redistributor</span></span>(<span>d, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the Redistributor or Redistributor_multi object to a file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_redistributor(d, path):
    &#34;&#34;&#34;Saves the Redistributor or Redistributor_multi object to a file.&#34;&#34;&#34;
    import joblib
    joblib.dump(d, path)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="redistributor.Redistributor"><code class="flex name class">
<span>class <span class="ident">Redistributor</span></span>
<span>(</span><span>target=&lt;scipy.stats._distn_infrastructure.rv_frozen object&gt;, known_distribution=None, bbox=None, closed_interval=True, prevent_same=True, tinity=1000000, validate_input=True, bins=0)</span>
</code></dt>
<dd>
<div class="desc"><p>An algorithm for automatic transformation of data from arbitrary
distribution into arbitrary distribution. Source distribution
can be known beforehandand or learned from the data. Transformation
is piecewise smooth, monotonic and invertible.</p>
<p>Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
(for more dimensions use Redistributor_multi wrapper) and saved to be used
later for transforming other data assuming the same source distribution.</p>
<p>Uses source's and target's cdf() and ppf() to infer the
transform and inverse transform functions.</p>
<p>transform_function = target_ppf(source_cdf(x))
inverse_transform = source_ppf(target_cdf(x))</p>
<h2 id="time-complexity">Time Complexity</h2>
<p>This algorithm can scale to a large number of samples &gt; 1e08.
Algorithm can be sped up by:
1. Turning off input data validation by setting validate_input=False
2. Choosing target distribution with fast cdf and ppf functions.
3. Specifying the source distribution if it is known.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target</code></strong> :&ensp;<code>obj</code>, optional, default <code>scipy.stats.norm(loc=0, scale=1)</code></dt>
<dd>Class specifying the target distribution. Must implement
cdf(), ppf() and ideally pdf() methods.
Continuous distributions from scipy.stats can be used.</dd>
<dt><strong><code>known_distribution</code></strong> :&ensp;<code>obj</code>, optional, default <code>None</code></dt>
<dd>Object specifying the source distribution when known.
Must implement cdf(), ppf() and ideally pdf() methods.
Continuous distributions from scipy.stats can be used.</dd>
<dt><strong><code>bbox</code></strong> :&ensp;<code>tuple</code>, optional, default <code>None</code></dt>
<dd>Tuple (a, b) which represent the lower and upper boundary
of the source distribution's domain.</dd>
<dt><strong><code>closed_interval</code></strong> :&ensp;<code>bool</code>, optional, default <code>True</code></dt>
<dd>If True, the values set in bounding box are included into
the interpolation range.</dd>
<dt><strong><code>prevent_same</code></strong> :&ensp;<code>bool</code>, optional, default <code>True</code></dt>
<dd>Flag, specifyig whether to prevent same values in the
interpolated vector.</dd>
<dt><strong><code>tinity</code></strong> :&ensp;<code>int</code>, default <code>1e06</code></dt>
<dd>Specifies divisor of noise added to vectors with same elements.
Bigger the tinity, smaller the noise.</dd>
<dt><strong><code>validate_input</code></strong> :&ensp;<code>bool</code>, optional, default <code>True</code></dt>
<dd>Flag specifying if the input data should be validated.
Turn off to save computation time. Output is undefined
if the data are not valid.</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code>, optional, default <code>0</code></dt>
<dd>
<p>Number of bins which are used to infer the latice density
when the distribution is learned. Irelevant with known_distribution.</p>
<p>If bins = x.size, then latice density = 100%. The interpolation step =
x.size // bins = 1. This is most precise. If x.size is big enough,
we can lower the latice density and the fit will remain very precise.</p>
<p>If bins = 0, it is set automatically. The latice density = 100%
is then used up to x.size = 5000. If x.size &gt; 5000, bins = 5000.</p>
</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>self.target_cdf: callable, default scipy.stats.norm(loc=0, scale=1).cdf
Cummulative Density Function of target distribution.</p>
<p>self.target_ppf: callable, default scipy.stats.norm(loc=0, scale=1).ppf
Percent Point Function (inverse of cdf) of target distribution.</p>
<p>self.source_cdf: callable, default None
Either known_distribution.cdf function of source distribution
or learned on fit() by interpolation on latice.</p>
<p>self.source_ppf: callable, default None
Either known_distribution.ppf function of source distribution
or learned on fit() by interpolation on latice.</p>
<p>self.a: float, default None
Beginning of the interval of the source distribution domain.</p>
<p>self.b: float, default None
End of the interval of the source distribution domain.</p>
<p>self.n: int, default None
Size of the training data extened by bounding box borders if necessary.</p>
<p>self.fitted: bool, default False
Flag that specifies whether the fit() was already called.</p>
<h2 id="limitations">Limitations</h2>
<ul>
<li>
<p>Output is undefined when most of the data points have the exact
same value, therefore data should not be sparse - multiple zeros etc.
Handles small amounts of the exact same values by adding tiny noise.</p>
</li>
<li>
<p>Plotting of learned pdf() is just a smoothed approximation obtained
by 1st derivative of the learned piece-wise smooth cdf() and it
serves only as a visual aid.</p>
</li>
</ul>
<h2 id="examples">Examples</h2>
<p>TODO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Redistributor(TransformerMixin):
    &#34;&#34;&#34;
    An algorithm for automatic transformation of data from arbitrary
    distribution into arbitrary distribution. Source distribution
    can be known beforehandand or learned from the data. Transformation
    is piecewise smooth, monotonic and invertible.

    Implemented as a Scikit-learn transformer. Can be fitted on 1D vector
    (for more dimensions use Redistributor_multi wrapper) and saved to be used
    later for transforming other data assuming the same source distribution.

    Uses source&#39;s and target&#39;s cdf() and ppf() to infer the
    transform and inverse transform functions.

    transform_function = target_ppf(source_cdf(x))
    inverse_transform = source_ppf(target_cdf(x))

    Time complexity
    --------
    This algorithm can scale to a large number of samples &gt; 1e08.
    Algorithm can be sped up by:
    1. Turning off input data validation by setting validate_input=False
    2. Choosing target distribution with fast cdf and ppf functions.
    3. Specifying the source distribution if it is known.

    Parameters
    --------
    target: obj, optional, default scipy.stats.norm(loc=0, scale=1)
        Class specifying the target distribution. Must implement
        cdf(), ppf() and ideally pdf() methods.
        Continuous distributions from scipy.stats can be used.

    known_distribution: obj, optional, default None
        Object specifying the source distribution when known.
        Must implement cdf(), ppf() and ideally pdf() methods.
        Continuous distributions from scipy.stats can be used.

    bbox: tuple, optional, default None
        Tuple (a, b) which represent the lower and upper boundary
        of the source distribution&#39;s domain.

    closed_interval: bool, optional, default True
        If True, the values set in bounding box are included into
        the interpolation range.

    prevent_same: bool, optional, default True
        Flag, specifyig whether to prevent same values in the
        interpolated vector.

    tinity: int, default 1e06
        Specifies divisor of noise added to vectors with same elements.
        Bigger the tinity, smaller the noise.

    validate_input: bool, optional, default True
        Flag specifying if the input data should be validated.
        Turn off to save computation time. Output is undefined
        if the data are not valid.

    bins: int, optional, default 0
        Number of bins which are used to infer the latice density
        when the distribution is learned. Irelevant with known_distribution.

        If bins = x.size, then latice density = 100%. The interpolation step =
        x.size // bins = 1. This is most precise. If x.size is big enough,
        we can lower the latice density and the fit will remain very precise.

        If bins = 0, it is set automatically. The latice density = 100%
        is then used up to x.size = 5000. If x.size &gt; 5000, bins = 5000.

    Attributes
    --------
    self.target_cdf: callable, default scipy.stats.norm(loc=0, scale=1).cdf
        Cummulative Density Function of target distribution.

    self.target_ppf: callable, default scipy.stats.norm(loc=0, scale=1).ppf
        Percent Point Function (inverse of cdf) of target distribution.

    self.source_cdf: callable, default None
        Either known_distribution.cdf function of source distribution
        or learned on fit() by interpolation on latice.

    self.source_ppf: callable, default None
        Either known_distribution.ppf function of source distribution
        or learned on fit() by interpolation on latice.

    self.a: float, default None
        Beginning of the interval of the source distribution domain.

    self.b: float, default None
        End of the interval of the source distribution domain.

    self.n: int, default None
        Size of the training data extened by bounding box borders if necessary.

    self.fitted: bool, default False
        Flag that specifies whether the fit() was already called.

    Limitations
    --------
    - Output is undefined when most of the data points have the exact
      same value, therefore data should not be sparse - multiple zeros etc.
      Handles small amounts of the exact same values by adding tiny noise.

    - Plotting of learned pdf() is just a smoothed approximation obtained
      by 1st derivative of the learned piece-wise smooth cdf() and it
      serves only as a visual aid.

    Examples
    --------
    TODO

    &#34;&#34;&#34;

    def __init__(self,
                 target=norm(loc=0, scale=1),
                 known_distribution=None,
                 bbox=None,
                 closed_interval=True,
                 prevent_same=True,
                 tinity=int(1e06),
                 validate_input=True,
                 bins=0):

        self.known_distribution = known_distribution
        self.bbox = bbox
        self.closed_interval = closed_interval
        self.prevent_same = prevent_same
        self.tinity = tinity
        self.validate_input = validate_input

        self.bins = bins  # Might chang on fit()
        self.target_cdf = target.cdf
        self.target_ppf = target.ppf

        # Will be computed or changed on fit()
        self.source_cdf = None
        self.source_ppf = None
        self.a = None
        self.b = None
        self.n = None
        self.fitted = False

        if self.known_distribution is not None:
            self.fit()

    def fit(self, x=None):
        &#34;&#34;&#34;
        Calls all necessary methods to infer cdf and ppf of the source
        distribution. Source distribution can be either specified directly by
        its function or learned from the training data on a closed or opened
        interval. Learning approximates the cdf and ppf by interpolating on a
        latice which density is infered from bins.

        Parameters
        --------

        x: 1D numpy array, optional, default None
            Training data from which the source distribution is learned.
            Must be specified if self.known_distribution is None.
        &#34;&#34;&#34;

        if self.known_distribution is not None:
            assert self.bbox is not None, \
                &#39;Bounding box must be specified when using known_distrubition.&#39;
            self._infer_a_b(x, self.bbox)
            try:
                self.source_cdf = self.known_distribution.cdf
                self.source_ppf = self.known_distribution.ppf
            except AttributeError:
                print(&#39;&#39;&#39;Class known_distribution must implement cdf(), ppf()
                and ideally pdf() methods.&#39;&#39;&#39;, file=sys.stderr)
                raise
            try:
                self.source_pdf = self.known_distribution.pdf
            except AttributeError:
                self.source_pdf = None
            self.known_distribution = self.known_distribution
        else:
            assert x is not None, \
                &#39;If known_distribution is None, X must be specified.&#39;

            # Calculate bounding box
            self._infer_a_b(x, self.bbox)

            # Validate input data
            if self.validate_input:
                self._validate_input(x)

            # Compute number of bins to use
            self._infer_nbins(x.size, self.bins)

            # Ensure, that a and b values are in x
            x = self._enforce_borders(x, self.closed_interval)

            # Get the size of x with borders a and b
            self.n = x.size

            # Learn the source distribution
            self._infer_cdf_ppf(x, self.prevent_same)

        self.fitted = True
        return self

    def transform(self, x):
        &#34;&#34;&#34;
        Applies learned transformation function to the data x.

        Parameters
        --------
        x : numpy array
            Data to transform. Must be within self.a, self.b interval.

        Returns
        --------
        Numpy array of transformed data.
        &#34;&#34;&#34;

        if self.validate_input:
            self._validate_input(x)
        return self.target_ppf(self.source_cdf(x))

    def inverse_transform(self, x):
        &#34;&#34;&#34;
        Applies learned inverse transform function to the data x.

        Parameters
        --------
        x : numpy array
            Data to inverse transform. Must be within the interp. interval.
            of learned transform function.

        Returns
        --------
        Numpy array of inverse transformed data.
        &#34;&#34;&#34;

        return self.source_ppf(self.target_cdf(x))

    def _validate_input(self, data):
        &#34;&#34;&#34;
        Validation of the input data used to validate inputs to fit() and
        transform(). Can be turned off globally by setting
        self.validate_input = False.

        Parameters
        --------
        data: numpy array of data to be validated
        &#34;&#34;&#34;
        assert data.dtype == np.float64 or data.dtype == np.float32, \
            &#39;Data must be float64 of 32.&#39;
        assert data.ndim == 1, \
            &#39;Data must be stored in 1D numpy array. You can use data.ravel().&#39;
        assert all(np.isfinite(data)), \
            &#39;Data must not contain np.nan or np.inf.&#39;
        assert self.a &lt;= np.min(data) and np.max(data) &lt;= self.b, \
            &#39;Data out of interval set by bbox.&#39;

    def _infer_a_b(self, data, bbox):
        &#34;&#34;&#34;
        Infers self.a and self.b which represent the lower and upper boundary
        of the source distribution&#39;s domain.

        Parameters
        --------
        data: numpy array or None
            Training data used to infer the a and b automaticaly by taking min
            and max. Can&#39;t be None if the bbox is None.

        bbox: tuple or None
            Tuple (a, b) which represent the lower and upper boundary
            of the source distribution&#39;s domain.
        &#34;&#34;&#34;

        if bbox is None:
            bbox = (np.min(data), np.max(data))
        assert len(bbox) == 2, \
            &#39;Please specify just two values in bbox.&#39;
        assert bbox[0] &lt; bbox[1], \
            &#39;First element of bbox must be smaller than the second.&#39;
        self.a = bbox[0]
        self.b = bbox[1]

    def _enforce_borders(self, x, closed_interval):
        &#34;&#34;&#34;
        Inserts self.a and self.b into the array x on the first and last
        positions respectively. If closed_interval=True, the values are
        extended by small amount, so the actual values a and b are still
        included in the interpolation range.

        Parameters
        --------
        x: numpy array
            1D vector into which the values are inserted.

        closed_interval: bool
            Flag, specifying the opened or closed interval.
        &#34;&#34;&#34;
        xmin = np.min(x)
        xmax = np.max(x)
        extend = (xmax - xmin) / x.size / 100 if closed_interval else 0
        if xmin != self.a - extend:
            x = np.insert(x, 0, self.a - extend)
        if xmax != self.b + extend:
            x = np.append(x, self.b + extend)
        return x

    def _infer_nbins(self, n, bins):
        &#34;&#34;&#34;
        Infers or validates the number of bins.

        Parameters
        --------
        n: int
            Size of the training data.

        bins:
            User specified value of bins.
        &#34;&#34;&#34;
        if bins == 0:
            bins = min(n, 5000)
        assert bins &gt; 2 and bins &lt;= n
        self.bins = bins

    def _prevent_same(self, x):
        &#34;&#34;&#34;
        Adds tiny noise to prevent same values in a vector if there are any.

        Parameters
        --------
        x: 1D numpy array
            Array with potential of having not unique elements.

        Returns
        --------
        Sorted array with added small noise based on self.tinity. The aim is
        to have all elements in the array unique.
        &#34;&#34;&#34;
        s = np.array(list(set(x)))
        if x.size != len(s):

            # Find the smallest distance between two consecutive elements
            mindist = np.abs(np.min(np.ediff1d(s))) if len(s) &gt; 1 else 1

            # Add tiny noise that is smaller than the smalledst distance
            tiny_noise = np.random.rand(x.size) * (mindist / self.tinity)
            x += tiny_noise
            x = np.sort(x)

            # Recursion for handling extremely rare case that there
            # are still some elements that are not unique
            return self._prevent_same(x)
        else:
            return x

    def _infer_cdf_ppf(self, x, prevent_same):
        &#34;&#34;&#34;
        Learning of the target distribution is done by linear interpolation
        on a latice. This method infers approximation of source&#39;s
        Cumulative distribution function and Percent point function.

        Parameters
        --------
        x: numpy array
            1D vector used for the interpolation.
        prevent_same: bool
            Flag, specifying whether to add small noise to values on the
            latice so the interpolation data do not have the exact same values.
            Applied only if there are some.
        &#34;&#34;&#34;

        # Define latice on which to interpolate, more points = bigger precision
        latice = np.linspace(0, self.n - 1, self.bins).astype(int)

        # Get values of x on latice points by partial sort
        x.partition(latice)
        values_on_latice = np.sort(x[latice])

        # Adds tiny noise to prevent same values if necessary
        # Leave first and last values untouched to retain bbox
        if prevent_same:
            values_on_latice[1:-1] = self._prevent_same(values_on_latice[1:-1])

        # Normalize the latice by nubmer of samples
        # Cummulative density function must sum up to 1
        latice = latice / (self.n - 1)

        # Interpolate to get cdf and ppf
        from scipy.interpolate import interp1d
        self.source_cdf = interp1d(values_on_latice, latice, kind=&#39;linear&#39;)
        self.source_ppf = interp1d(latice, values_on_latice, kind=&#39;linear&#39;)

    def compute_empirical_cdf_error(self, x, error_func=&#39;mse&#39;):
        &#34;&#34;&#34;
        Computes error caused by approximation of transform function.
        Error computation needs to sort all data. It can take a long time
        for big arrays.

        Parameters
        --------
        x: numpy array
            1D vector of transformed data.
        error_func: callable or one of {&#39;mae&#39;, &#39;mse&#39;}
            Callable that computes error on two vectors.
            &#39;mae&#39; = Error in L1 norm (Mean Absolute Error)
            &#39;mse&#39; = Error in L2 norm (Mean Squared Error)

        Returns
        --------
        Float value of specified error or return value of callable.
        &#34;&#34;&#34;

        if error_func == &#39;mse&#39;:
            from sklearn.metrics import mean_squared_error as errfunc
        elif error_func == &#39;mae&#39;:
            from sklearn.metrics import mean_absolute_error as errfunc
        else:
            errfunc = error_func
            assert callable(errfunc), \
                &#39;Set error_func to &#34;mse&#34;, &#34;mae&#34; or func that computes error.&#39;

        transformed = self.transform(x)
        transformed = np.sort(transformed)
        n = transformed.size
        p_lower = self.target_cdf(transformed[0])
        p_upper = self.target_cdf(transformed[-1])
        v = p_lower + ((p_upper - p_lower) / (n - 1)) * np.arange(0, n)
        w = self.target_cdf(transformed)
        return errfunc(v, w)

    def plot_transform_function(self, figsize=(15, 2)):
        &#34;&#34;&#34;
        Displays matplotlib plot of the fitted transform function.

        Parameters
        --------
        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.
        &#34;&#34;&#34;
        assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

        x_axis = np.linspace(self.a, self.b, 1000)
        transformed = self.transform(x_axis)

        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        plt.plot(x_axis, transformed)
        plt.title(&#39;Learned monotonic piecewise smooth transform function&#39;)
        plt.xlim(self.a, self.b)
        plt.show()
        plt.close()

    def plot_source_pdf(self, smoothed=True, figsize=(15, 2)):
        &#34;&#34;&#34;
        Displays matplotlib plot of the learned source probability density
        function.

        Parameters
        --------
        smoothed: boolean, optional, default True
            Applies savgol_filter to smooth the curve that is disturbed by
            derivatives at bin transitions.

        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.
        &#34;&#34;&#34;
        assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

        steps = 1000
        step = (self.b - self.a) / steps

        x_axis = np.linspace(self.a, self.b, steps, endpoint=False)
        x_axis = x_axis[1:]

        if self.known_distribution:
            if self.source_pdf is not None:
                curve = self.source_pdf(x_axis)
                title = &#39;Pdf of known distribution.&#39;
                text = None
            else:
                print(&#39;&#39;&#39;Can&#39;t plot source pdf. Class of known distribution
                does not implement pdf().&#39;&#39;&#39;, file=sys.stderr)
                return
        else:
            from scipy.misc import derivative
            curve = derivative(self.source_cdf, x_axis, dx=step / 2)
            sm = &#39;&#39;
            if smoothed:
                from scipy.signal import savgol_filter
                win_width = steps // 16
                if win_width % 2 == 0:
                    win_width += 1
                curve = savgol_filter(curve, win_width, 1)
                sm = &#39; (smoothed)&#39;
            title = &#39;Source_pdf{} approximated using {} bins.&#39;.format(
                sm, self.bins)
            text = &#39;&#39;&#39;\nMay not show appropriate results with certain source
             distributions. In which case rather plot a histogram of your train
             data.&#39;&#39;&#39;

        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        plt.plot(x_axis, curve)
        plt.title(title)
        plt.xlim(self.a, self.b)
        plt.ylim((0., 1.2 * np.max(curve)))
        if text is not None:
            ax = plt.gca()
            plt.text(0.5, -0.4,
                     text,
                     size=10,
                     ha=&#39;center&#39;,
                     va=&#39;bottom&#39;,
                     transform=ax.transAxes)
        plt.show()
        plt.close()

    def plot_hist(self, data, nbins=None, title=&#39;Histogram&#39;,
                  figsize=(15, 2), xlim=None):
        &#34;&#34;&#34;
        Displays matplotlib histogram of the specified data and nbins.

        Parameters
        --------
        data : numpy array
            Specifying the position of data points on x axis.

        nbins : int, optional, default self.bins
            Specifying the number of bins of the histogram.
            If None, the number will be automatically set by matplotlib.

        title : str, optional, default &#39;Histogram&#39;
            Title of the plot.

        figsize : tuple (width, height), optional, default (15,2)
            Desired size of the figure.

        xlim : float, optional, default None
            Limit of x axis.
        &#34;&#34;&#34;
        import matplotlib.pyplot as plt
        plt.figure(figsize=figsize)
        if self.known_distribution is None:
            if nbins is None and self.n &gt; 2500:
                nbins = min(self.bins, 301)
        if xlim is not None:
            plt.xlim(xlim)
        plt.hist(data, nbins)
        plt.title(title)
        plt.show()
        plt.close()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="redistributor.Redistributor.compute_empirical_cdf_error"><code class="name flex">
<span>def <span class="ident">compute_empirical_cdf_error</span></span>(<span>self, x, error_func='mse')</span>
</code></dt>
<dd>
<div class="desc"><p>Computes error caused by approximation of transform function.
Error computation needs to sort all data. It can take a long time
for big arrays.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>1D vector of transformed data.</dd>
<dt><strong><code>error_func</code></strong> :&ensp;<code>callable</code> or <code>one</code> of <code>{'mae', 'mse'}</code></dt>
<dd>Callable that computes error on two vectors.
'mae' = Error in L1 norm (Mean Absolute Error)
'mse' = Error in L2 norm (Mean Squared Error)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Float value of specified error or return value of callable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_empirical_cdf_error(self, x, error_func=&#39;mse&#39;):
    &#34;&#34;&#34;
    Computes error caused by approximation of transform function.
    Error computation needs to sort all data. It can take a long time
    for big arrays.

    Parameters
    --------
    x: numpy array
        1D vector of transformed data.
    error_func: callable or one of {&#39;mae&#39;, &#39;mse&#39;}
        Callable that computes error on two vectors.
        &#39;mae&#39; = Error in L1 norm (Mean Absolute Error)
        &#39;mse&#39; = Error in L2 norm (Mean Squared Error)

    Returns
    --------
    Float value of specified error or return value of callable.
    &#34;&#34;&#34;

    if error_func == &#39;mse&#39;:
        from sklearn.metrics import mean_squared_error as errfunc
    elif error_func == &#39;mae&#39;:
        from sklearn.metrics import mean_absolute_error as errfunc
    else:
        errfunc = error_func
        assert callable(errfunc), \
            &#39;Set error_func to &#34;mse&#34;, &#34;mae&#34; or func that computes error.&#39;

    transformed = self.transform(x)
    transformed = np.sort(transformed)
    n = transformed.size
    p_lower = self.target_cdf(transformed[0])
    p_upper = self.target_cdf(transformed[-1])
    v = p_lower + ((p_upper - p_lower) / (n - 1)) * np.arange(0, n)
    w = self.target_cdf(transformed)
    return errfunc(v, w)</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls all necessary methods to infer cdf and ppf of the source
distribution. Source distribution can be either specified directly by
its function or learned from the training data on a closed or opened
interval. Learning approximates the cdf and ppf by interpolating on a
latice which density is infered from bins.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D numpy array</code>, optional, default <code>None</code></dt>
<dd>Training data from which the source distribution is learned.
Must be specified if self.known_distribution is None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x=None):
    &#34;&#34;&#34;
    Calls all necessary methods to infer cdf and ppf of the source
    distribution. Source distribution can be either specified directly by
    its function or learned from the training data on a closed or opened
    interval. Learning approximates the cdf and ppf by interpolating on a
    latice which density is infered from bins.

    Parameters
    --------

    x: 1D numpy array, optional, default None
        Training data from which the source distribution is learned.
        Must be specified if self.known_distribution is None.
    &#34;&#34;&#34;

    if self.known_distribution is not None:
        assert self.bbox is not None, \
            &#39;Bounding box must be specified when using known_distrubition.&#39;
        self._infer_a_b(x, self.bbox)
        try:
            self.source_cdf = self.known_distribution.cdf
            self.source_ppf = self.known_distribution.ppf
        except AttributeError:
            print(&#39;&#39;&#39;Class known_distribution must implement cdf(), ppf()
            and ideally pdf() methods.&#39;&#39;&#39;, file=sys.stderr)
            raise
        try:
            self.source_pdf = self.known_distribution.pdf
        except AttributeError:
            self.source_pdf = None
        self.known_distribution = self.known_distribution
    else:
        assert x is not None, \
            &#39;If known_distribution is None, X must be specified.&#39;

        # Calculate bounding box
        self._infer_a_b(x, self.bbox)

        # Validate input data
        if self.validate_input:
            self._validate_input(x)

        # Compute number of bins to use
        self._infer_nbins(x.size, self.bins)

        # Ensure, that a and b values are in x
        x = self._enforce_borders(x, self.closed_interval)

        # Get the size of x with borders a and b
        self.n = x.size

        # Learn the source distribution
        self._infer_cdf_ppf(x, self.prevent_same)

    self.fitted = True
    return self</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies learned inverse transform function to the data x.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Data to inverse transform. Must be within the interp. interval.
of learned transform function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Numpy array of inverse transformed data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, x):
    &#34;&#34;&#34;
    Applies learned inverse transform function to the data x.

    Parameters
    --------
    x : numpy array
        Data to inverse transform. Must be within the interp. interval.
        of learned transform function.

    Returns
    --------
    Numpy array of inverse transformed data.
    &#34;&#34;&#34;

    return self.source_ppf(self.target_cdf(x))</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.plot_hist"><code class="name flex">
<span>def <span class="ident">plot_hist</span></span>(<span>self, data, nbins=None, title='Histogram', figsize=(15, 2), xlim=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Displays matplotlib histogram of the specified data and nbins.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Specifying the position of data points on x axis.</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code>, optional, default <code>self.bins</code></dt>
<dd>Specifying the number of bins of the histogram.
If None, the number will be automatically set by matplotlib.</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional, default <code>'Histogram'</code></dt>
<dd>Title of the plot.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (width, height)</code>, optional, default <code>(15,2)</code></dt>
<dd>Desired size of the figure.</dd>
<dt><strong><code>xlim</code></strong> :&ensp;<code>float</code>, optional, default <code>None</code></dt>
<dd>Limit of x axis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_hist(self, data, nbins=None, title=&#39;Histogram&#39;,
              figsize=(15, 2), xlim=None):
    &#34;&#34;&#34;
    Displays matplotlib histogram of the specified data and nbins.

    Parameters
    --------
    data : numpy array
        Specifying the position of data points on x axis.

    nbins : int, optional, default self.bins
        Specifying the number of bins of the histogram.
        If None, the number will be automatically set by matplotlib.

    title : str, optional, default &#39;Histogram&#39;
        Title of the plot.

    figsize : tuple (width, height), optional, default (15,2)
        Desired size of the figure.

    xlim : float, optional, default None
        Limit of x axis.
    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    plt.figure(figsize=figsize)
    if self.known_distribution is None:
        if nbins is None and self.n &gt; 2500:
            nbins = min(self.bins, 301)
    if xlim is not None:
        plt.xlim(xlim)
    plt.hist(data, nbins)
    plt.title(title)
    plt.show()
    plt.close()</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.plot_source_pdf"><code class="name flex">
<span>def <span class="ident">plot_source_pdf</span></span>(<span>self, smoothed=True, figsize=(15, 2))</span>
</code></dt>
<dd>
<div class="desc"><p>Displays matplotlib plot of the learned source probability density
function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>smoothed</code></strong> :&ensp;<code>boolean</code>, optional, default <code>True</code></dt>
<dd>Applies savgol_filter to smooth the curve that is disturbed by
derivatives at bin transitions.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (width, height)</code>, optional, default <code>(15,2)</code></dt>
<dd>Desired size of the figure.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_source_pdf(self, smoothed=True, figsize=(15, 2)):
    &#34;&#34;&#34;
    Displays matplotlib plot of the learned source probability density
    function.

    Parameters
    --------
    smoothed: boolean, optional, default True
        Applies savgol_filter to smooth the curve that is disturbed by
        derivatives at bin transitions.

    figsize : tuple (width, height), optional, default (15,2)
        Desired size of the figure.
    &#34;&#34;&#34;
    assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

    steps = 1000
    step = (self.b - self.a) / steps

    x_axis = np.linspace(self.a, self.b, steps, endpoint=False)
    x_axis = x_axis[1:]

    if self.known_distribution:
        if self.source_pdf is not None:
            curve = self.source_pdf(x_axis)
            title = &#39;Pdf of known distribution.&#39;
            text = None
        else:
            print(&#39;&#39;&#39;Can&#39;t plot source pdf. Class of known distribution
            does not implement pdf().&#39;&#39;&#39;, file=sys.stderr)
            return
    else:
        from scipy.misc import derivative
        curve = derivative(self.source_cdf, x_axis, dx=step / 2)
        sm = &#39;&#39;
        if smoothed:
            from scipy.signal import savgol_filter
            win_width = steps // 16
            if win_width % 2 == 0:
                win_width += 1
            curve = savgol_filter(curve, win_width, 1)
            sm = &#39; (smoothed)&#39;
        title = &#39;Source_pdf{} approximated using {} bins.&#39;.format(
            sm, self.bins)
        text = &#39;&#39;&#39;\nMay not show appropriate results with certain source
         distributions. In which case rather plot a histogram of your train
         data.&#39;&#39;&#39;

    import matplotlib.pyplot as plt
    plt.figure(figsize=figsize)
    plt.plot(x_axis, curve)
    plt.title(title)
    plt.xlim(self.a, self.b)
    plt.ylim((0., 1.2 * np.max(curve)))
    if text is not None:
        ax = plt.gca()
        plt.text(0.5, -0.4,
                 text,
                 size=10,
                 ha=&#39;center&#39;,
                 va=&#39;bottom&#39;,
                 transform=ax.transAxes)
    plt.show()
    plt.close()</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.plot_transform_function"><code class="name flex">
<span>def <span class="ident">plot_transform_function</span></span>(<span>self, figsize=(15, 2))</span>
</code></dt>
<dd>
<div class="desc"><p>Displays matplotlib plot of the fitted transform function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple (width, height)</code>, optional, default <code>(15,2)</code></dt>
<dd>Desired size of the figure.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_transform_function(self, figsize=(15, 2)):
    &#34;&#34;&#34;
    Displays matplotlib plot of the fitted transform function.

    Parameters
    --------
    figsize : tuple (width, height), optional, default (15,2)
        Desired size of the figure.
    &#34;&#34;&#34;
    assert self.fitted, &#39;First, the object must be fitted using fit().&#39;

    x_axis = np.linspace(self.a, self.b, 1000)
    transformed = self.transform(x_axis)

    import matplotlib.pyplot as plt
    plt.figure(figsize=figsize)
    plt.plot(x_axis, transformed)
    plt.title(&#39;Learned monotonic piecewise smooth transform function&#39;)
    plt.xlim(self.a, self.b)
    plt.show()
    plt.close()</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies learned transformation function to the data x.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Data to transform. Must be within self.a, self.b interval.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Numpy array of transformed data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x):
    &#34;&#34;&#34;
    Applies learned transformation function to the data x.

    Parameters
    --------
    x : numpy array
        Data to transform. Must be within self.a, self.b interval.

    Returns
    --------
    Numpy array of transformed data.
    &#34;&#34;&#34;

    if self.validate_input:
        self._validate_input(x)
    return self.target_ppf(self.source_cdf(x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="redistributor.Redistributor_multi"><code class="flex name class">
<span>class <span class="ident">Redistributor_multi</span></span>
<span>(</span><span>redistributors, nsub, cpus=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-dimensional wrapper for Redistributor.
Allows to use multiple Redistributors on equal-sized slices (submatrices)
of N-dimensional input array. Utilizes parallel processing with low memory
footprint.</p>
<h2 id="parameters">Parameters:</h2>
<p>redistributors: numpy array of Redistributor objects
Objects that will be used to operate on the data within each slice
defined by nsub. Shape must be the same as nsub.</p>
<p>nsub: tuple of int
Tuple specifying how to split multidimensional input
into submatrices. Corresponding redistributor object will
be applied on each submatrix separately. Each int
specifies to how many equal submatrices corresponding
axis should be split. There must be exactly one int
for each axis of multidimensional input data.</p>
<p>cpus: int &gt;= 0
Number of cpu cores to use in multiprocessing.
If 0, all cores will be used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Redistributor_multi():
    &#34;&#34;&#34;
    Multi-dimensional wrapper for Redistributor.
    Allows to use multiple Redistributors on equal-sized slices (submatrices)
    of N-dimensional input array. Utilizes parallel processing with low memory
    footprint.

    Parameters:
    --------

    redistributors: numpy array of Redistributor objects
        Objects that will be used to operate on the data within each slice
        defined by nsub. Shape must be the same as nsub.

    nsub: tuple of int
        Tuple specifying how to split multidimensional input
        into submatrices. Corresponding redistributor object will
        be applied on each submatrix separately. Each int
        specifies to how many equal submatrices corresponding
        axis should be split. There must be exactly one int
        for each axis of multidimensional input data.

    cpus: int &gt;= 0
        Number of cpu cores to use in multiprocessing.
        If 0, all cores will be used.
    &#34;&#34;&#34;

    def __init__(self, redistributors, nsub, cpus=0):
        self.nsub = np.array(nsub)
        if np.prod(self.nsub == 1):
            print(&#39;&#39;&#39;WARNING: Using Redistributor_multi on whole matrix is
                slower than just Redistributor.&#39;&#39;&#39;, file=sys.stderr)
        self.redistributors = redistributors
        assert self.redistributors.size == np.prod(
            self.nsub), &#39;Specify one redistributor per slice.&#39;
        self.cpus = self._infer_cpus(cpus)
        self.fitted = False

    def _infer_cpus(self, cpus):
        &#34;&#34;&#34;
        Keeps the defined number of cpus or tries to set it to all
        available cpus if cpus = 0.
        &#34;&#34;&#34;
        if cpus == 0:
            try:
                from multiprocessing import cpu_count
                return cpu_count()
            except NotImplementedError:
                return 1  # default
        else:
            return cpus

    def fit(self, x=None, size_limit=0):
        &#34;&#34;&#34;
        Fits all Redistributor objects in self.redistributors.

        Parameters
        --------
        x : numpy array, optional, default None
            Data to fit on. If None, Redistributors must have
            source set explicitly.

        size_limit : float, optional, default 0
            Should be 3x smaller than available memory after loading the data
            in GB. Based on this value, the appropriate size of chunk is
            infered. If size_limit == 0, the best value is computed
            automatically.
        &#34;&#34;&#34;
        if x is not None:
            self.x = x
            self.machinery(&#39;fit&#39;, size_limit=size_limit)
        self.fitted = True

    def transform(self, x, inplace=True, size_limit=0):
        &#34;&#34;&#34;
        Transforms data in x using fitted Redistributors.

        Parameters
        --------
        x : numpy array
            Data to transform.

        inplace : bool, optional, default True
            Flag specifying whether to change the data in place without
            keeping the original values stored in x or operate on copy.

        size_limit : float, optional, default 0
            See docstring of self.fit

        Returns
        --------
        Transformed data with same shape as input.
        &#34;&#34;&#34;
        self.inplace = inplace
        self.x = x if self.inplace else x.copy()
        return self.machinery(&#39;transform&#39;, size_limit=size_limit)

    def inverse_transform(self, x, inplace=True, size_limit=0):
        &#34;&#34;&#34;
        Inverse transforms the data in x using fitted Redistributors.

        Parameters
        --------
        x : numpy array
            Data to transform.

        inplace : bool, optional, default True
            Flag specifying whether to change the data in place without
            keeping the original values stored in x or operate on copy.

        size_limit : float, optional, default 0
            See docstring of self.fit

        Returns
        --------
        Inverse transformed data with same shape as input.
        &#34;&#34;&#34;
        self.inplace = inplace
        self.x = x if self.inplace else x.copy()
        return self.machinery(&#39;inverse&#39;, size_limit=size_limit)

    def _locate_subarrays(self, xshape, nsub):
        &#34;&#34;&#34;
        Locates subarrays within matrix x according to nsub.

        Returns
        --------
        - Numpy array of indices that locate the subarrays within matrix x.
        - Shape of subarray that would be produced by slicing the matrix.
        Each subarray is of equal shape.
        &#34;&#34;&#34;

        nsub = np.array(nsub)
        shape = np.array(xshape)
        steps, rems = np.divmod(shape, nsub)
        assert all(rems == 0), \
            &#39;Nsub does not divide the x equally on {} axes.&#39;.format(
                np.nonzero(rems)[0])
        steps = np.array(shape / nsub).astype(int)
        output_shape = steps

        subarray_indices = [list(zip(
            range(0, shape[axis], steps[axis]),
            range(0 + steps[axis], shape[axis] + steps[axis], steps[axis])))
            for axis in range(len(nsub))]

        return (np.array(list(itertools.product(*subarray_indices))),
                output_shape)

    def _indices_to_slices(self, indices):
        &#34;&#34;&#34;
        Converts np array of start and stop indices to np array of slices.
        &#34;&#34;&#34;
        shape = indices.shape
        assert shape[-1] in (2, 3), \
            &#39;indices.shape[-1] must be 2 or 3 for (start, stop, [step]).&#39;
        indices = indices.reshape(-1, 2)
        return np.array([slice(*ind) for ind in indices]).reshape(shape[:-1])

    def _get_size_limit(self):
        &#34;&#34;&#34;
        Checks available memory and decides on size_limit for
        self._get_chunksize.
        &#34;&#34;&#34;
        try:
            import psutil
            size_limit = psutil.virtual_memory().available / 2.2e09
            if size_limit &lt; 0.1:
                print(&#39;&#39;&#39;WARNING: It seems you have too low available memory.
                The speed might be influenced significantly. For optimal speed
                it is good to have ~2x size_limit of free memory after loading
                tha data that are being processed. Size_limit for one chunk of
                sharedctypes array was set to default 0.5GB. Consider using
                self.cpus = 1.&#39;&#39;&#39;, file=sys.stderr)
                return 0.5
            else:
                return size_limit
        except:
            import sys
            print(&#39;&#39;&#39;WARNING: Unable to obtain the size of available memory.
            Size_limit for one chunk of sharedctypes array was set to default
            0.5GB. For optimal speed it is good to have ~2x size_limit of free
            memory after loading tha data that are being processed.&#39;&#39;&#39;,
                  file=sys.stderr)
            return 0.5

    def _get_chunksize(self, size_limit):
        &#34;&#34;&#34;
        Returns number of slices that should be in one chunk so the size of
        chunk is ideally equal to size_limit. Divides all slices to chunks
        with agreement to self.nsub. The size of chunk will be bigger if it is
        not possible. No matter the size, it never returns 1, because the whole
        class looses its meaning. Returns 1 only if the np.prod(self.nsub) == 1
        which is discouraged.

        Parameters
        --------
        size_limit : float
            Ideal size of one chunk so the sharedctypes array used
            in multiprocessing pipe in self.machinery is created as fast
            as possible.
        &#34;&#34;&#34;
        all_slices = np.prod(self.nsub)
        last_best = 1
        slice_size = self.x.nbytes / all_slices * 1e-09  # in GB
        for i, n in enumerate(self.nsub):
            if n == 1:
                continue
            returnOne = False if np.prod(self.nsub[i + 1:]) == 1 else True
            possible_steps_on_axis = np.array(list(reversed(sorted(_divisors(
                n, returnOne=returnOne, returnX=True)))))
            slices_in_chunk = possible_steps_on_axis * np.prod(
                self.nsub[i + 1:])
            for s in slices_in_chunk:
                if s * slice_size &lt;= size_limit:
                    return s
                last_best = s
        return last_best

    @staticmethod
    def init_global_array(array):
        &#34;&#34;&#34;Initializer of shared array for multiprocessing pool.&#34;&#34;&#34;
        global arr
        arr = array

    @staticmethod
    def populate(args):
        &#34;&#34;&#34;
        Static method called by child processes that applies desired
        function of redistributor on the data from shared matrix on
        desired location and populates the result back to the shared matrix.
        &#34;&#34;&#34;
        index, location, shape, redistributor, purpose, cpus = args
        if cpus == 1:
            # Get the access to the global array
            matrix = arr
        else:
            # Get the access to shraed array
            matrix = np.ctypeslib.as_array(arr)

        # Take the vector from the shared array
        v = matrix[tuple(location)].ravel()

        if purpose == &#39;fit&#39;:
            redistributor.fit(v)
        elif purpose == &#39;transform&#39;:
            matrix[tuple(location)] = redistributor.transform(v).reshape(shape)
        elif purpose == &#39;inverse&#39;:
            matrix[tuple(location)] = redistributor.inverse_transform(
                v).reshape(shape)

        return index, redistributor

    def machinery(self, purpose, size_limit):
        &#34;&#34;&#34;
        Handles locating subarrays and their parallel processing in chunks.

        Parameters
        --------
        purpose : one of {&#39;fit&#39;, &#39;transform&#39;, &#39;inverse&#39;}
            Specifies what should be done with the data.

        size_limit : float
            Ideal size of one chunk. 0 = automatic.
        &#34;&#34;&#34;

        # Number of all subarrays that will be used
        n_subarrays = np.prod(self.nsub)

        # Get subarray locations (list of indices)
        indices, output_shape = self._locate_subarrays(self.x.shape, self.nsub)

        # Avoiding multiprocessing and the overhead of creating shared_array
        if self.cpus == 1:
            locations = self._indices_to_slices(indices).tolist()
            Redistributor_multi.init_global_array(self.x)
            list(map(Redistributor_multi.populate,
                     zip(range(len(locations)),
                         locations,
                         itertools.repeat(output_shape),
                         self.redistributors.ravel(),
                         itertools.repeat(purpose),
                         itertools.repeat(self.cpus))))

        # Using pool of child processes running in parallel
        else:
            from multiprocessing import Pool
            from multiprocessing import RawArray

            # Get chunksize for splitting the self.x so creation of
            # shared_array is faster
            if size_limit == 0:
                size_limit = self._get_size_limit()
            chunksize = self._get_chunksize(size_limit)
            stepsize = n_subarrays // chunksize

            chunked_indices = indices.reshape(stepsize, -1, len(self.nsub), 2)
            locations_of_subarrays_within_each_chunk = self._indices_to_slices(
                chunked_indices[0]).tolist()

            for i, chunks_subarray_indices in enumerate(chunked_indices):
                # Find start and stop of chunk slice in each axis
                mins = np.min(chunks_subarray_indices, axis=2)[0]
                maxs = np.max(chunks_subarray_indices, axis=2)[-1]
                indices_of_chunk = np.array(list(zip(mins, maxs)))
                location_of_chunk = self._indices_to_slices(
                    indices_of_chunk).tolist()

                # Create a shared memory array that is accessible by the
                # child processes
                s = self.x[tuple(location_of_chunk)].copy()
                tmp = np.ctypeslib.as_ctypes(s)

                # Creating a shared array if much faster when the underlying
                # C code can make a copy of it. If there is not that much
                # available memory left it replaces value by value in place
                # which takes significantly more time. That is the reason this
                # is chunkized into smaller arrays and done in a for loop.
                shared_array = RawArray(tmp._type_, tmp)
                del s
                del tmp

                pool = Pool(processes=self.cpus,
                            initializer=Redistributor_multi.init_global_array,
                            initargs=(shared_array, ))

                p = pool.map(Redistributor_multi.populate,
                             zip(range(i * chunksize, (i + 1) * chunksize),
                                 locations_of_subarrays_within_each_chunk,
                                 itertools.repeat(output_shape),
                                 self.redistributors.ravel(),
                                 itertools.repeat(purpose),
                                 itertools.repeat(self.cpus)))

                # Update redistributor objects after being changed
                [np.put(self.redistributors, i, instance) for i, instance in p]

                pool.close()
                pool.join()

                if purpose != &#39;fit&#39;:
                    self.x[tuple(location_of_chunk)] = np.ctypeslib.as_array(
                        shared_array)

                # Freeing the memory
                del shared_array

        # Returning the results
        if purpose == &#39;fit&#39;:
            output = None
        else:
            output = self.x

        # Cleaning up the object
        del self.x
        return output</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="redistributor.Redistributor_multi.init_global_array"><code class="name flex">
<span>def <span class="ident">init_global_array</span></span>(<span>array)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializer of shared array for multiprocessing pool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def init_global_array(array):
    &#34;&#34;&#34;Initializer of shared array for multiprocessing pool.&#34;&#34;&#34;
    global arr
    arr = array</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor_multi.populate"><code class="name flex">
<span>def <span class="ident">populate</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>Static method called by child processes that applies desired
function of redistributor on the data from shared matrix on
desired location and populates the result back to the shared matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def populate(args):
    &#34;&#34;&#34;
    Static method called by child processes that applies desired
    function of redistributor on the data from shared matrix on
    desired location and populates the result back to the shared matrix.
    &#34;&#34;&#34;
    index, location, shape, redistributor, purpose, cpus = args
    if cpus == 1:
        # Get the access to the global array
        matrix = arr
    else:
        # Get the access to shraed array
        matrix = np.ctypeslib.as_array(arr)

    # Take the vector from the shared array
    v = matrix[tuple(location)].ravel()

    if purpose == &#39;fit&#39;:
        redistributor.fit(v)
    elif purpose == &#39;transform&#39;:
        matrix[tuple(location)] = redistributor.transform(v).reshape(shape)
    elif purpose == &#39;inverse&#39;:
        matrix[tuple(location)] = redistributor.inverse_transform(
            v).reshape(shape)

    return index, redistributor</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="redistributor.Redistributor_multi.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x=None, size_limit=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits all Redistributor objects in self.redistributors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code>, optional, default <code>None</code></dt>
<dd>Data to fit on. If None, Redistributors must have
source set explicitly.</dd>
<dt><strong><code>size_limit</code></strong> :&ensp;<code>float</code>, optional, default <code>0</code></dt>
<dd>Should be 3x smaller than available memory after loading the data
in GB. Based on this value, the appropriate size of chunk is
infered. If size_limit == 0, the best value is computed
automatically.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x=None, size_limit=0):
    &#34;&#34;&#34;
    Fits all Redistributor objects in self.redistributors.

    Parameters
    --------
    x : numpy array, optional, default None
        Data to fit on. If None, Redistributors must have
        source set explicitly.

    size_limit : float, optional, default 0
        Should be 3x smaller than available memory after loading the data
        in GB. Based on this value, the appropriate size of chunk is
        infered. If size_limit == 0, the best value is computed
        automatically.
    &#34;&#34;&#34;
    if x is not None:
        self.x = x
        self.machinery(&#39;fit&#39;, size_limit=size_limit)
    self.fitted = True</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor_multi.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, x, inplace=True, size_limit=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse transforms the data in x using fitted Redistributors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Data to transform.</dd>
<dt><strong><code>inplace</code></strong> :&ensp;<code>bool</code>, optional, default <code>True</code></dt>
<dd>Flag specifying whether to change the data in place without
keeping the original values stored in x or operate on copy.</dd>
<dt><strong><code>size_limit</code></strong> :&ensp;<code>float</code>, optional, default <code>0</code></dt>
<dd>See docstring of self.fit</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Inverse transformed data with same shape as input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, x, inplace=True, size_limit=0):
    &#34;&#34;&#34;
    Inverse transforms the data in x using fitted Redistributors.

    Parameters
    --------
    x : numpy array
        Data to transform.

    inplace : bool, optional, default True
        Flag specifying whether to change the data in place without
        keeping the original values stored in x or operate on copy.

    size_limit : float, optional, default 0
        See docstring of self.fit

    Returns
    --------
    Inverse transformed data with same shape as input.
    &#34;&#34;&#34;
    self.inplace = inplace
    self.x = x if self.inplace else x.copy()
    return self.machinery(&#39;inverse&#39;, size_limit=size_limit)</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor_multi.machinery"><code class="name flex">
<span>def <span class="ident">machinery</span></span>(<span>self, purpose, size_limit)</span>
</code></dt>
<dd>
<div class="desc"><p>Handles locating subarrays and their parallel processing in chunks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>purpose</code></strong> :&ensp;<code>one</code> of <code>{'fit', 'transform', 'inverse'}</code></dt>
<dd>Specifies what should be done with the data.</dd>
<dt><strong><code>size_limit</code></strong> :&ensp;<code>float</code></dt>
<dd>Ideal size of one chunk. 0 = automatic.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def machinery(self, purpose, size_limit):
    &#34;&#34;&#34;
    Handles locating subarrays and their parallel processing in chunks.

    Parameters
    --------
    purpose : one of {&#39;fit&#39;, &#39;transform&#39;, &#39;inverse&#39;}
        Specifies what should be done with the data.

    size_limit : float
        Ideal size of one chunk. 0 = automatic.
    &#34;&#34;&#34;

    # Number of all subarrays that will be used
    n_subarrays = np.prod(self.nsub)

    # Get subarray locations (list of indices)
    indices, output_shape = self._locate_subarrays(self.x.shape, self.nsub)

    # Avoiding multiprocessing and the overhead of creating shared_array
    if self.cpus == 1:
        locations = self._indices_to_slices(indices).tolist()
        Redistributor_multi.init_global_array(self.x)
        list(map(Redistributor_multi.populate,
                 zip(range(len(locations)),
                     locations,
                     itertools.repeat(output_shape),
                     self.redistributors.ravel(),
                     itertools.repeat(purpose),
                     itertools.repeat(self.cpus))))

    # Using pool of child processes running in parallel
    else:
        from multiprocessing import Pool
        from multiprocessing import RawArray

        # Get chunksize for splitting the self.x so creation of
        # shared_array is faster
        if size_limit == 0:
            size_limit = self._get_size_limit()
        chunksize = self._get_chunksize(size_limit)
        stepsize = n_subarrays // chunksize

        chunked_indices = indices.reshape(stepsize, -1, len(self.nsub), 2)
        locations_of_subarrays_within_each_chunk = self._indices_to_slices(
            chunked_indices[0]).tolist()

        for i, chunks_subarray_indices in enumerate(chunked_indices):
            # Find start and stop of chunk slice in each axis
            mins = np.min(chunks_subarray_indices, axis=2)[0]
            maxs = np.max(chunks_subarray_indices, axis=2)[-1]
            indices_of_chunk = np.array(list(zip(mins, maxs)))
            location_of_chunk = self._indices_to_slices(
                indices_of_chunk).tolist()

            # Create a shared memory array that is accessible by the
            # child processes
            s = self.x[tuple(location_of_chunk)].copy()
            tmp = np.ctypeslib.as_ctypes(s)

            # Creating a shared array if much faster when the underlying
            # C code can make a copy of it. If there is not that much
            # available memory left it replaces value by value in place
            # which takes significantly more time. That is the reason this
            # is chunkized into smaller arrays and done in a for loop.
            shared_array = RawArray(tmp._type_, tmp)
            del s
            del tmp

            pool = Pool(processes=self.cpus,
                        initializer=Redistributor_multi.init_global_array,
                        initargs=(shared_array, ))

            p = pool.map(Redistributor_multi.populate,
                         zip(range(i * chunksize, (i + 1) * chunksize),
                             locations_of_subarrays_within_each_chunk,
                             itertools.repeat(output_shape),
                             self.redistributors.ravel(),
                             itertools.repeat(purpose),
                             itertools.repeat(self.cpus)))

            # Update redistributor objects after being changed
            [np.put(self.redistributors, i, instance) for i, instance in p]

            pool.close()
            pool.join()

            if purpose != &#39;fit&#39;:
                self.x[tuple(location_of_chunk)] = np.ctypeslib.as_array(
                    shared_array)

            # Freeing the memory
            del shared_array

    # Returning the results
    if purpose == &#39;fit&#39;:
        output = None
    else:
        output = self.x

    # Cleaning up the object
    del self.x
    return output</code></pre>
</details>
</dd>
<dt id="redistributor.Redistributor_multi.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x, inplace=True, size_limit=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms data in x using fitted Redistributors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Data to transform.</dd>
<dt><strong><code>inplace</code></strong> :&ensp;<code>bool</code>, optional, default <code>True</code></dt>
<dd>Flag specifying whether to change the data in place without
keeping the original values stored in x or operate on copy.</dd>
<dt><strong><code>size_limit</code></strong> :&ensp;<code>float</code>, optional, default <code>0</code></dt>
<dd>See docstring of self.fit</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Transformed data with same shape as input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x, inplace=True, size_limit=0):
    &#34;&#34;&#34;
    Transforms data in x using fitted Redistributors.

    Parameters
    --------
    x : numpy array
        Data to transform.

    inplace : bool, optional, default True
        Flag specifying whether to change the data in place without
        keeping the original values stored in x or operate on copy.

    size_limit : float, optional, default 0
        See docstring of self.fit

    Returns
    --------
    Transformed data with same shape as input.
    &#34;&#34;&#34;
    self.inplace = inplace
    self.x = x if self.inplace else x.copy()
    return self.machinery(&#39;transform&#39;, size_limit=size_limit)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#compatibility">Compatibility</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#mathematical-description">Mathematical description</a></li>
<li><a href="#how-to-cite">How to cite</a></li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgement">Acknowledgement</a></li>
<li><a href="#to-do-list">To do list</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="redistributor.load_redistributor" href="#redistributor.load_redistributor">load_redistributor</a></code></li>
<li><code><a title="redistributor.save_redistributor" href="#redistributor.save_redistributor">save_redistributor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="redistributor.Redistributor" href="#redistributor.Redistributor">Redistributor</a></code></h4>
<ul class="">
<li><code><a title="redistributor.Redistributor.compute_empirical_cdf_error" href="#redistributor.Redistributor.compute_empirical_cdf_error">compute_empirical_cdf_error</a></code></li>
<li><code><a title="redistributor.Redistributor.fit" href="#redistributor.Redistributor.fit">fit</a></code></li>
<li><code><a title="redistributor.Redistributor.inverse_transform" href="#redistributor.Redistributor.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="redistributor.Redistributor.plot_hist" href="#redistributor.Redistributor.plot_hist">plot_hist</a></code></li>
<li><code><a title="redistributor.Redistributor.plot_source_pdf" href="#redistributor.Redistributor.plot_source_pdf">plot_source_pdf</a></code></li>
<li><code><a title="redistributor.Redistributor.plot_transform_function" href="#redistributor.Redistributor.plot_transform_function">plot_transform_function</a></code></li>
<li><code><a title="redistributor.Redistributor.transform" href="#redistributor.Redistributor.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="redistributor.Redistributor_multi" href="#redistributor.Redistributor_multi">Redistributor_multi</a></code></h4>
<ul class="two-column">
<li><code><a title="redistributor.Redistributor_multi.fit" href="#redistributor.Redistributor_multi.fit">fit</a></code></li>
<li><code><a title="redistributor.Redistributor_multi.init_global_array" href="#redistributor.Redistributor_multi.init_global_array">init_global_array</a></code></li>
<li><code><a title="redistributor.Redistributor_multi.inverse_transform" href="#redistributor.Redistributor_multi.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="redistributor.Redistributor_multi.machinery" href="#redistributor.Redistributor_multi.machinery">machinery</a></code></li>
<li><code><a title="redistributor.Redistributor_multi.populate" href="#redistributor.Redistributor_multi.populate">populate</a></code></li>
<li><code><a title="redistributor.Redistributor_multi.transform" href="#redistributor.Redistributor_multi.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>